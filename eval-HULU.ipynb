{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b605c15-fe38-4905-96d7-6fc9301966ba",
   "metadata": {},
   "source": [
    "# HuLU eval of Koboldcpp hosted LLM\n",
    "HuLU dataset is created with similar goals in mind as GLUE, but for Hungarian specific language evaluation in mind. This means, it is just a classification task list, which required to be evaluated on any kind of NLP classifier you can throw at it.\n",
    "\n",
    "It's intended workflow is the following:\n",
    "- One trains the classifier with training set from the whole dataset\n",
    "- Checks back with the evaluation dataset how it performs (it is also labeled)\n",
    "- Then runs the whole Test set, and stores the results (it doesn't contain labels, it's private)\n",
    "- Uploads the result to somewhere, which calculates the metrics, and possibly displays it's score on a leader board\n",
    "\n",
    "## Why keep the test labels secret?\n",
    "Simple enough: to prevent the LLMs to contaminate their knowledge with the correct results, thus preventing some LLM developers to artificially train their LLMs with the good results, thus advancing their creation on the leader board.\n",
    "\n",
    "Though, so far I didn't came accross any leader boards, nor services which calculates the metrics for me.\n",
    "\n",
    "## How to evaluate then?\n",
    "The idea, is to use the evaluation part of the dataset, to measure. We skip the part, where we train the LLM for the classification part, and instead we use the context learning capability of the model: we give a short description of the work they need to perform, and give some examples (5 diverse examples to be concrate) from the training set. This gives us a few-shot-example-prompt, which according to my experience, most of the LLMs can easily comply. For this, to work, the most interresting candidates for evaluation, are the instruction finetuned versions of the models.\n",
    "\n",
    "## Why use Kobobldcpp to host LLM?\n",
    "Koboldcpp is easy to configure through command line, can use CPU, CUDA, OpenCL, etc, so it's easy to squeeze out most of what's available on the local, or remote machine, without touching this very notebook, and it's codes. Also, one can use any arbitrary quantized version of any openly available LLM, provided Koboldcpp is capable to run it. It it can run, a lot of different types of LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f28fe92-34a5-4ccd-83d9-bbe414917fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.0.1)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.32.3)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (3.10.9)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.25.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests) (2024.6.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (1.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\cybor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)\n",
      "   ---------------------------------------- 0.0/44.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.9/44.5 MB 19.6 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 8.7/44.5 MB 21.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 14.4/44.5 MB 23.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 20.7/44.5 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 26.7/44.5 MB 26.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 33.6/44.5 MB 27.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.6/44.5 MB 27.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.3/44.5 MB 28.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.5/44.5 MB 26.5 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.5.2-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 6.8/11.0 MB 35.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 28.6 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, evaluate\n",
      "Successfully installed evaluate-0.4.3 joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate requests scipy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f329a8a-934e-4916-9b7f-d7231f85e5e0",
   "metadata": {},
   "source": [
    "Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f66cb71-794e-490a-b64a-9adb67ea03ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40349319-3430-4f9c-93f8-d2b6c3968c6c",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to evaluate one of the [Transformers](https://github.com/huggingface/transformers) model on [HuLU Benchmark](https://hulu.nytud.hu/) dataset.\n",
    "\n",
    "The HuLU Benchmark is a group of six classification tasks on sentences or pairs of sentences which are:\n",
    "\n",
    "- [HuCoLA](https://github.com/nytud/HuCOLA) (Hungarian Corpus of Linguistic Acceptability) contains 9 076 Hungarian sentences labeled for their acceptability/grammaticality (0/1).\n",
    "- [HuCoPA](https://github.com/nytud/HuCoPA) (Hungarian Choice of Plausible Alternatives Corpus) contains 1,000 instances. Each instance is composed of a premise and two alternatives. The task is to select the alternative that describes a situation standing in causal relation to the situation described by the premise.\n",
    "- [HuCB](https://github.com/nytud/HuCommitmentBank) The HuCommitmentBank consists of short text fragments in which at least one sentence contains a subordinating clause, which is syntactically subordinated to a logical inference-cancelling operator.\n",
    "- [HuRTE](https://github.com/nytud/HuRTE) (Hungarian Recognizing Textual Entailment) The dataset contains 4 504 instances. Each example contains a (sometimes multi-sentence) premise and a one-sentence hypothesis, and the task is to decide whether the former entails the latter or not.Determine if a sentence entails a given hypothesis or not.\n",
    "- [HuSST](https://github.com/nytud/HuSST) (Hungarian version of the Stanford Sentiment Treebank) contains 11 683 sentences. Each sentence is annotated for its sentiment on a three-point scale.\n",
    "- [HuWNLI](https://github.com/nytud/HuWNLI) (Winograd Natural Language Inference) Anaphora resolution datasets for Hungarian as an inference task; this is a Hungarian dataset of anaphora resolution, designed as a sentence pair classification task of natural language inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeeb854-c914-443e-8d0d-becdd3d6079c",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ec648-694f-4662-820e-43fb6f272f1d",
   "metadata": {},
   "source": [
    "We will use git clone to download data, and [Datasets](https://github.com/huggingface/datasets) library to load the data and [Evaluate](https://github.com/huggingface/evaluate) library to get the metric we need to use for evaluation (to compare our model to the benchmark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce30e634-e1f1-40a0-aeef-be8fae7ed285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'hulu/hucola'...\n",
      "Cloning into 'hulu/hucopa'...\n",
      "Cloning into 'hulu/hucb'...\n",
      "Cloning into 'hulu/hurte'...\n",
      "Cloning into 'hulu/husst'...\n",
      "Cloning into 'hulu/huwnli'...\n"
     ]
    }
   ],
   "source": [
    "!mkdir hulu\n",
    "!git clone https://github.com/nytud/HuCOLA/ hulu/hucola\n",
    "!git clone https://github.com/nytud/HuCoPA/ hulu/hucopa\n",
    "!git clone https://github.com/nytud/HuCommitmentBank/ hulu/hucb\n",
    "!git clone https://github.com/nytud/HuRTE/ hulu/hurte\n",
    "!git clone https://github.com/nytud/HuSST/ hulu/husst\n",
    "!git clone https://github.com/nytud/HuWNLI/ hulu/huwnli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e7593-23f1-429f-9d52-39d11c9a2b48",
   "metadata": {},
   "source": [
    "### Define tasks\n",
    "What can be found where, and which metric belongs to which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45377c7-7f69-4c11-90ff-b99a3f9e907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HULU_TASKS = [\n",
    "    (\"hucola\", \"hulu/hucola/data/cola_\", [\"train\", \"dev\", \"test\"], \"cola\"), \n",
    "    (\"hucopa\", \"hulu/hucopa/data/\", [\"train\", \"val\", \"test\"], \"rte\"), \n",
    "    (\"hucb\", \"hulu/hucb/data/hucb_\", [\"train\", \"dev\", \"test\"], \"mnli\"), \n",
    "    (\"hurte\", \"hulu/hurte/data/rte_\", [\"train\", \"dev\", \"test\"], \"rte\"), \n",
    "    (\"husst\", \"hulu/husst/data/sst_\", [\"train\", \"dev\", \"test\"], \"sst2\"), \n",
    "    (\"huwnli\", \"hulu/huwnli/data/\", [\"train\", \"dev\", \"test\"], \"wnli\")\n",
    "]\n",
    "task = \"hucb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b34461-67d4-41f1-a9cd-fb92227a3454",
   "metadata": {},
   "source": [
    "**Note**: I had a little problem reading in '''hucb''' json files (deserialization errors), and the solution was to open them in editor, and save back with UTF-8 marker chars in the beginning of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c317c8df-c8f6-4bba-95aa-3eb017f02517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.75}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'premise', 'hypothesis', 'label'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "for (actual_task, path, variants, glue_metric) in HULU_TASKS:\n",
    "    if actual_task == task:\n",
    "        dataset = load_dataset('json', data_files=f\"{path}{variants[0]}.json\")\n",
    "        metric = load_metric('glue', glue_metric)\n",
    "        break\n",
    "\n",
    "references = [0, 1, 0, 1]\n",
    "predictions = [0, 1, 1, 1]\n",
    "results = metric.compute(predictions=predictions, references=references)\n",
    "print(results)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ac3b9-033a-4929-bdb6-be28ca70dfdb",
   "metadata": {},
   "source": [
    "## Output values\n",
    "\n",
    "The output of the metric depends on the GLUE subset chosen, consisting of a dictionary that contains one or several of the following metrics:\n",
    "\n",
    "`accuracy`: the proportion of correct predictions among the total number of cases processed, with a range between 0 and 1 (see [accuracy](https://huggingface.co/metrics/accuracy) for more information).\n",
    "\n",
    "`f1`: the harmonic mean of the precision and recall (see [F1 score](https://huggingface.co/metrics/f1) for more information). Its range is 0-1 – its lowest possible value is 0, if either the precision or the recall is 0, and its highest possible value is 1.0, which means perfect precision and recall.\n",
    "\n",
    "`pearson`: a measure of the linear relationship between two datasets (see [Pearson correlation](https://huggingface.co/metrics/pearsonr) for more information). Its range is between -1 and +1, with 0 implying no correlation, and -1/+1 implying an exact linear relationship. Positive correlations imply that as x increases, so does y, whereas negative correlations imply that as x increases, y decreases.\n",
    "\n",
    "`spearmanr`: a nonparametric measure of the monotonicity of the relationship between two datasets(see [Spearman Correlation](https://huggingface.co/metrics/spearmanr) for more information). spearmanr has the same range as pearson.\n",
    "\n",
    "`matthews_correlation`: a measure of the quality of binary and multiclass classifications (see [Matthews Correlation](https://huggingface.co/metrics/matthews_correlation) for more information). Its range of values is between -1 and +1, where a coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction.\n",
    "\n",
    "The cola subset returns matthews_correlation, the stsb subset returns pearson and spearmanr, the mrpc and qqp subsets return both accuracy and f1, and all other subsets of GLUE return only accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3d51fa5-850d-4e5a-9867-d03654a09a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: f\"{typ.names[i]} ({i})\")\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7aca74-3887-4565-9d3b-a5081df6897f",
   "metadata": {},
   "source": [
    "We can visualize a random portion of the loaded dataset by calling the above defined method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52f5ebcc-d93a-49d9-9bdf-95389cbe35c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>247</td>\n",
       "      <td>Asszonyszíveken különösen ninc s Bramah-závárJól van – szólt Iván. – Tehát tegyük fel, hogy sikerülend a herceget is, meg a grófot is rábírni, hogy a birtokot eladják; még akkor mindig nincsen nagyszerű établissement-od.</td>\n",
       "      <td>A beszélő szerint rábírja a birtok eladására a grófot.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>Azt gondolták, hogy ez a kormány nem lesz cselekvőképes, és a költségvetési folyamatban el fog bukni. Akkor lett szerintem új helyzet, amikor elfogadták a költségvetést. Világossá válhatott mindenki számára, hogy ilyen politikai stratégia nem működőképes. Egyik oldalról van egy szükség tárgyalni, hisz az elmúlt ötven-hatvan év legnagyobb válsága előtt állunk. Másik oldalról pedig a korábbi szándékok úgy látom, hogy nem érvényesíthetők.</td>\n",
       "      <td>A beszélő szerint nem érvényesíthetők a korábbi szándékok.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115</td>\n",
       "      <td>András: - Az új diplomámat is egyelőre a fiókba tettem. Riporter: És mi lesz pár év múlva, mi lesz pár év múlva, amikor a gyerekek már iskolások lesznek? András: - Egyelőre még konkrétumon nem gondolkodtam...  ...a diplomámmal kapcsolatban nagyon valószínűnek tartom, hogy egy egzakt, fix munkahelyre, munkaidőbe nem fogok tudni visszamenni, de a saját temperamentumomat is ismerve, én úgy képzelem el, hogy valami kicsi vállalkozást fogok megint kitalálni és abban fogom kiélni a vágyaimat.</td>\n",
       "      <td>A beszélő szerint valami kicsi vállalkozást fog megint kitalálni.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113</td>\n",
       "      <td>Az önkormányzat a saját tulajdonával, ha másként nem, merthogy ő szabályozza a területet, hogy mit lehet ott csinálni, mit nem, - maga rendelkezik. Ebben az államnak sok dolga nincsen, de van persze az érmnek másik oldala, hogy épültek aztán teniszpályák, meg hát épültek lovardák, ahogyan itt a beszélgetés legelején erről szó volt. Műsorvezető: - Igen ám! Csak van itt egy apró probléma, hogy én ugyan nem vagyok sportriporter, de valahogy úgy látom, vagy úgy képzelem el, hogy a sportélet, az valamiféle piramis.</td>\n",
       "      <td>A beszélő szerint a sportélet olyan, mint egy piramis.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149</td>\n",
       "      <td>Gabriella: - A világ ember nélkül. Egy csodálatos fotóalbum a National Geographic kiadásában. Egyed László: - Nagyon sok oka lehet annak, ha az ember nem tesz feljelentést egy ellene elkövetett bűncselekmény miatt. Lehet ez félelem, szégyen, vagy az hogy valaki nem bízik abban, hogy bármi féle eredménye lesz a feljelentésnek, vagy megkerül a bűnös.</td>\n",
       "      <td>A beszélő szerint lesz eredménye a feljelentésnek.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>204</td>\n",
       "      <td>Műsorvezető: - Tehát magyarán nyomást gyakorolnak a kormányra kívülről. Weinhardt Attila: - Én azt gondolom, hogy igen, ez is szóba fog kerülni, sőt én akár arra is látok esélyt, hogy később, január, február, március során módosítani kell a gazdasági stabilitásról szóló törvényt, és esetleg azon a jegybanktörvény tervezeten is, amelyről pénteken szavaz majd a tisztelt ház. Műsorvezető: - Tehát ön azt mondja, hogy még így, a módosításokkal sem lesz elfogadható az Európai Központi Bank az unió illetve az IMF számára a jegybank törvény?</td>\n",
       "      <td>A műsorvezető szerint a módosításokkal együtt sem lesz elfogadható a jegybanktörvény az Európai Központi Bank, az Unió illetve az IMF számára.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>68</td>\n",
       "      <td>Műsorvezető: - Még akkor sem, ha azt mondjuk, hogy megosztottság ide vagy oda, politikai gyűlölködés ide vagy oda, alapelvekben, demokratikus alapelvekben egy jogállam - már bocsánat, hogy ezt a szót használom, bár Ön szerint ez rosszul van az alkotmányban, illetve hogy nem jelent semmit, vagy nem azt jelenti... Szentpéteri-Nagy Richárd, a Méltányosabb Politika Nemzetközpont munkatársa: - Tehát hogy pontatlanul van... Műsorvezető: -... igen, tehát hogy ebben... jelző kérdésekben meg lehetett volna állapodni, nem hiszem, hogy egyik párt abban különbözne a másiktól, hogy demokratikus alapjogokat akar sérteni, vagy nem akar védeni!</td>\n",
       "      <td>Szentpéteri-Nagy Richárd szerint egyik párt abban különbözik a másiktól, hogy demokratikus alapjogokat akar sérteni, vagy nem akar védeni.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>155</td>\n",
       "      <td>Én erre látok esélyt, mert saját tapasztalatom az, hogy megváltozott ennek az édesanyának az alapvetően az egész sorsa, életkörülményei mások, tehát amit mondtam, rendes családi házban élnek, én ott voltam is, normális körülmények között nevelik a kisgyermeket, akiről egyébként a helyi óvoda olyan véleményt adott, hogy kifejezetten jó véleményt, tehát hogy értelmes, beilleszkedő, társai szeretik, jól érzi ott magát, hát állítólag az idézőjelbe vett nagymama ugye azt mondta, hogy sírt is a gyerek, amikor nagy hirtelen megjelentek ott rajtaütésszerűen, és elvitték a gyereket, igazából érzelmileg is én úgy gondolom, hogy én a család mellé tudtam állni, és érzelmileg is mindamellett, hogy jogilag is tudom védeni az álláspontomat, érzelmileg is az ő álláspontjukon vagyok, és körömmel képviselem ezt az ügyet. Riporter: - Akkor ezek szerint bízik abban, hogy Renáta visszakaphatja kisfiát, Rolandot?</td>\n",
       "      <td>A Riporter szerint Renáta visszakaphatja kisfiát, Rolandot.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>Ha minden második családban még egy gyerek születne akkor tudnánk az öregedést leállítani. Nem szaporodna a népesség, csak éppen leállna az elöregedési folyamat, ami megmenthetné a nyugdíjrendszert az egészségügyet úgy ahogy van. másrészt ne felejtsük el hogy a gyermekes családok a belföldi vállalkozások piacát jelentik.</td>\n",
       "      <td>A beszélő szerint a gyermekes családok a belföldi vállalkozások piacát jelentik.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>126</td>\n",
       "      <td>Kézzel fejik, de felfőzik, megoltsák, úgy készítik ők is. Nem csak úgy egyszerűen meglesz a sajt, azt meg kell szűrni, azt meg kell melegíteni. Tesznek bele oltót, ami összeállítsa. Riporter: -  És elhiszi, hogy az uniós szabályozásba nem fér bele ez a háztáji, háznál gyártott termék, mint a juhsajt, vagy a juhtúró?</td>\n",
       "      <td>A beszélő szerint az uniós szabályozásba nem fér bele ez a háztáji, háznál gyártott termék, mint a juhsajt, vagy a juhtúró.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3197b46-9236-4d21-b574-57c3304b04db",
   "metadata": {},
   "source": [
    "## Task description\n",
    "Here we define a task description for each of the tasks:\n",
    "- Where the data can be found in the dataset\n",
    "- What needs the LLM do with those\n",
    "- Regexp for the result to parse for\n",
    "\n",
    "Out of these a prompt will be constructed for each of the tasks, automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1aff3bc6-ffc2-4caf-ab90-c966e89eb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"hucola\": (\"Sent\", None, None, None, \"Határozd meg, hogy a mondat nyelvtanilag helyes-e (1), vagy helytelen (0).\", r'[01]'),\n",
    "    \"hucopa\": (\"premise\", \"choice1\", \"choice2\", \"question\", \"Válaszd ki, melyik mondat (1 vagy 2) a kérdésnek megfelelő válasz.\", r'[12]'),\n",
    "    \"hucb\": (\"premise\", \"hypothesis\", None, None, \"Határozd meg, hogy a premisa és hipotézis milyen kapcsolatban állnak: ellentmondás (0), semleges (1), következmény (2)\", r'[012]'),\n",
    "    \"hurte\": (\"premise\", \"hypothesis\", None, None, \"Határozd meg, hogy a premisából következik-e (1) a hipotézis, vagy sem (0).\", r'[01]'),\n",
    "    \"husst\": (\"Sent\", None, None, None, \"Határozd meg, hogy a mondat pozitív (1) vagy negatív (0) hangulatú-e.\", r'[01]'),\n",
    "    \"huwnli\": (\"sentence1\", \"sentence2\", None, None, \"Határozd meg, hogy az első mondatból következik-e (1) a második mondat, vagy sem (0).\", r'[01]'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a981d21-ab3c-4c38-a547-6ce5c24ffb4a",
   "metadata": {},
   "source": [
    "## The prompt\n",
    "To prepare the prompt, we provide a system description, comming from `task_to_keys`, and additional 5 examples from the training dataset, with scores included.\n",
    "\n",
    "The actual test is comming from the \"validation\" dataset part, one by one, only providing the sentences to operate on, and simply prompting for a numerical answer by placing <|assistant|> at the line start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71e4b095-ba36-4fc5-b5e9-99d67f55fbec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def find_first_number(input_str, matchstr):\n",
    "    match = re.search(matchstr, input_str)\n",
    "    return float(match.group()) if match else None\n",
    "\n",
    "def get_label_value(input_str):\n",
    "    num = find_first_number(input_str, r'\\d+(\\.\\d+)?')\n",
    "    if not num is None:\n",
    "        return num\n",
    "    if input_str == \"contradiction\" or input_str == \"negative\":\n",
    "        return 0\n",
    "    elif input_str == \"neutral\" or input_str == \"positive\":\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "def measure(actual_task, path, variants, glue_metric):\n",
    "    (field1, field2, field3, field4, system_prompt, matchstr) = task_to_keys[actual_task]\n",
    "    dataset = load_dataset('json', data_files=f\"{path}{variants[0]}.json\")\n",
    "    metric = load_metric('glue', glue_metric)\n",
    "    \n",
    "    prompt = f\"<|system|>{system_prompt}\\n\"\n",
    "    training_data = dataset['train']\n",
    "    \n",
    "    # Here we collect a diverse labled training set of 5 to make a few shot examples\n",
    "    last_label = 0\n",
    "    num_examples = 0\n",
    "    for idx, f1 in enumerate(training_data[field1]):\n",
    "        if training_data['label'][idx] == last_label: continue\n",
    "        prompt += f\"<|{field1 if field1 != \"Sent\" else \"sentence\"}|>{f1}\\n\"\n",
    "        if not field2 is None:\n",
    "            prompt += f\"<|{field2}|>{training_data[field2][idx]}\\n\"\n",
    "        if not field3 is None:\n",
    "            prompt += f\"<|{field3}|>{training_data[field3][idx]}\\n\"\n",
    "        if not field4 is None:\n",
    "            if training_data[field4][idx] == \"cause\":\n",
    "                prompt += f\"<|{field4}|>indok\\n\"\n",
    "            else:\n",
    "                prompt += f\"<|{field4}|>következmény\\n\"\n",
    "        prompt += f\"<|assistant|>{get_label_value(training_data['label'][idx])}\\n\"\n",
    "        \n",
    "        last_label = training_data['label'][idx]\n",
    "        num_examples += 1\n",
    "        if num_examples >= 5: break\n",
    "    \n",
    "    api_url = \"http://localhost:5001/api/v1\"\n",
    "    stop_words = [\"###\",\"**Observation**\",\"</s>\",\"<|\"]\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    references = []\n",
    "    predictions = []\n",
    "    failed = []\n",
    "    testset = load_dataset('json', data_files=f\"{path}{variants[1]}.json\")\n",
    "    testset = testset['train']\n",
    "    count = 0\n",
    "    for idx, f1 in enumerate(testset[field1]):\n",
    "        query = prompt\n",
    "        query += f\"<|{field1 if field1 != \"Sent\" else \"sentence\"}|>{f1}\\n\"\n",
    "        if not field2 is None:\n",
    "            query += f\"<|{field2}|>{training_data[field2][idx]}\\n\"\n",
    "        if not field3 is None:\n",
    "            query += f\"<|{field3}|>{training_data[field3][idx]}\\n\"\n",
    "        if not field4 is None:\n",
    "            if training_data[field4][idx] == \"cause\":\n",
    "                query += f\"<|{field4}|>indok\\n\"\n",
    "            else:\n",
    "                query += f\"<|{field4}|>következmény\\n\"\n",
    "        query += f\"<|assistant|>\"\n",
    "        \n",
    "        data = {\n",
    "            \"prompt\": query,\n",
    "            \"max_tokens\": 10,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"n\": 20,\n",
    "            \"stop\": stop_words\n",
    "        }\n",
    "        \n",
    "        response = requests.post(f\"{api_url}/completion\", headers=headers, json=data)\n",
    "        result = response.json()[\"choices\"][0][\"text\"]\n",
    "        for sw in stop_words:\n",
    "            result = result.replace(sw, \"\")\n",
    "        result = result.strip()\n",
    "        num_result = find_first_number(result, matchstr)\n",
    "        if num_result is None:\n",
    "            failed.append(idx)\n",
    "        else:\n",
    "            references.append(get_label_value(testset['label'][idx]))\n",
    "            predictions.append(num_result)\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print(f\"Task: {actual_task}, {count/len(testset[field1])*100:5.1f}%\", end=\"\\r\")\n",
    "    \n",
    "    results = metric.compute(predictions=predictions, references=references)\n",
    "    return (len(testset[field1]), len(failed), results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbbd4271-8728-4f81-bb49-e07f5d2adf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: hucb 103/0               \n",
      "{'accuracy': 0.3786407766990291}\n"
     ]
    }
   ],
   "source": [
    "for (actual_task, path, variants, glue_metric) in HULU_TASKS:\n",
    "    if actual_task == task:\n",
    "        (number, failed, result) = measure(task, path, variants, glue_metric)\n",
    "        break\n",
    "\n",
    "print(f\"Task: {task} {number}/{failed}               \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35765694-f85f-4a57-a67c-2b9802a9b157",
   "metadata": {},
   "source": [
    "# Putting all together\n",
    "- iterate through all tasks\n",
    "- load related dataset & metric\n",
    "- evaluate all verification elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f26a84fb-03f4-4133-aefa-2bede06d86b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: hucola 910/0               \n",
      "{'matthews_correlation': 0.05182714475181878}\n",
      "Task: hucopa 100/0               \n",
      "{'accuracy': 0.5}\n",
      "Task: hucb 103/0               \n",
      "{'accuracy': 0.39805825242718446}\n",
      "Task: hurte 243/0               \n",
      "{'accuracy': 0.448559670781893}\n",
      "Task: husst 1165/0               \n",
      "{'accuracy': 0.6678111587982832}\n",
      "Task: huwnli 60/0               \n",
      "{'accuracy': 0.55}\n"
     ]
    }
   ],
   "source": [
    "for (task, path, variants, glue_metric) in HULU_TASKS:\n",
    "    (number, failed, result) = measure(task, path, variants, glue_metric)\n",
    "    print(f\"Task: {task} {number}/{failed}               \")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf759954-8158-4d50-abf9-d86fb561f4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
