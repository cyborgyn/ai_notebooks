{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b605c15-fe38-4905-96d7-6fc9301966ba",
   "metadata": {},
   "source": [
    "# GLUE eval of Koboldcpp hosted LLM\n",
    "Install required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192ee022-1506-4633-8d58-e5ae2a306d39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f28fe92-34a5-4ccd-83d9-bbe414917fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install deepeval datasets evaluate requests scipy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f329a8a-934e-4916-9b7f-d7231f85e5e0",
   "metadata": {},
   "source": [
    "Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1f66cb71-794e-490a-b64a-9adb67ea03ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40349319-3430-4f9c-93f8-d2b6c3968c6c",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to evaluate one of the [Transformers](https://github.com/huggingface/transformers) model on [HuLU Benchmark](https://hulu.nytud.hu/) dataset.\n",
    "\n",
    "The HuLU Benchmark is a group of six classification tasks on sentences or pairs of sentences which are:\n",
    "\n",
    "- [HuCoLA](https://github.com/nytud/HuCOLA) (Hungarian Corpus of Linguistic Acceptability) contains 9 076 Hungarian sentences labeled for their acceptability/grammaticality (0/1).\n",
    "- [HuCoPA](https://github.com/nytud/HuCoPA) (Hungarian Choice of Plausible Alternatives Corpus) contains 1,000 instances. Each instance is composed of a premise and two alternatives. The task is to select the alternative that describes a situation standing in causal relation to the situation described by the premise.\n",
    "- [HuCB](https://github.com/nytud/HuCommitmentBank) The HuCommitmentBank consists of short text fragments in which at least one sentence contains a subordinating clause, which is syntactically subordinated to a logical inference-cancelling operator.\n",
    "- [HuRTE](https://github.com/nytud/HuRTE) (Hungarian Recognizing Textual Entailment) The dataset contains 4 504 instances. Each example contains a (sometimes multi-sentence) premise and a one-sentence hypothesis, and the task is to decide whether the former entails the latter or not.Determine if a sentence entails a given hypothesis or not.\n",
    "- [HuSST](https://github.com/nytud/HuSST) (Hungarian version of the Stanford Sentiment Treebank) contains 11 683 sentences. Each sentence is annotated for its sentiment on a three-point scale.\n",
    "- [HuWNLI](https://github.com/nytud/HuWNLI) (Winograd Natural Language Inference) Anaphora resolution datasets for Hungarian as an inference task; this is a Hungarian dataset of anaphora resolution, designed as a sentence pair classification task of natural language inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeeb854-c914-443e-8d0d-becdd3d6079c",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ec648-694f-4662-820e-43fb6f272f1d",
   "metadata": {},
   "source": [
    "We will use git clone to download data, and [Datasets](https://github.com/huggingface/datasets) library to load the data and [Evaluate](https://github.com/huggingface/evaluate) library to get the metric we need to use for evaluation (to compare our model to the benchmark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce30e634-e1f1-40a0-aeef-be8fae7ed285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'hulu/hucola'...\n",
      "remote: Enumerating objects: 136, done.\u001b[K\n",
      "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
      "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
      "remote: Total 136 (delta 9), reused 0 (delta 0), pack-reused 116 (from 1)\u001b[K\n",
      "Receiving objects: 100% (136/136), 719.81 KiB | 5.22 MiB/s, done.\n",
      "Resolving deltas: 100% (62/62), done.\n",
      "Cloning into 'hulu/hucopa'...\n",
      "remote: Enumerating objects: 98, done.\u001b[K\n",
      "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
      "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
      "remote: Total 98 (delta 42), reused 12 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (98/98), 234.20 KiB | 3.16 MiB/s, done.\n",
      "Resolving deltas: 100% (42/42), done.\n",
      "Cloning into 'hulu/hucb'...\n",
      "remote: Enumerating objects: 43, done.\u001b[K\n",
      "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
      "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
      "remote: Total 43 (delta 18), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (43/43), 147.36 KiB | 1.94 MiB/s, done.\n",
      "Resolving deltas: 100% (18/18), done.\n",
      "Cloning into 'hulu/hurte'...\n",
      "remote: Enumerating objects: 42, done.\u001b[K\n",
      "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
      "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
      "remote: Total 42 (delta 17), reused 6 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (42/42), 670.97 KiB | 5.08 MiB/s, done.\n",
      "Resolving deltas: 100% (17/17), done.\n",
      "Cloning into 'hulu/husst'...\n",
      "remote: Enumerating objects: 81, done.\u001b[K\n",
      "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
      "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
      "remote: Total 81 (delta 32), reused 18 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (81/81), 1.22 MiB | 7.88 MiB/s, done.\n",
      "Resolving deltas: 100% (32/32), done.\n",
      "Cloning into 'hulu/huwnli'...\n",
      "remote: Enumerating objects: 136, done.\u001b[K\n",
      "remote: Counting objects: 100% (131/131), done.\u001b[K\n",
      "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
      "remote: Total 136 (delta 61), reused 122 (delta 56), pack-reused 5 (from 1)\u001b[K\n",
      "Receiving objects: 100% (136/136), 154.37 KiB | 1.95 MiB/s, done.\n",
      "Resolving deltas: 100% (61/61), done.\n"
     ]
    }
   ],
   "source": [
    "!mkdir hulu\n",
    "!git clone https://github.com/nytud/HuCOLA/ hulu/hucola\n",
    "!git clone https://github.com/nytud/HuCoPA/ hulu/hucopa\n",
    "!git clone https://github.com/nytud/HuCommitmentBank/ hulu/hucb\n",
    "!git clone https://github.com/nytud/HuRTE/ hulu/hurte\n",
    "!git clone https://github.com/nytud/HuSST/ hulu/husst\n",
    "!git clone https://github.com/nytud/HuWNLI/ hulu/huwnli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e7593-23f1-429f-9d52-39d11c9a2b48",
   "metadata": {},
   "source": [
    "### Define tasks\n",
    "What can be found where, and which metric belongs to which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b45377c7-7f69-4c11-90ff-b99a3f9e907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HULU_TASKS = [\n",
    "    (\"hucola\", \"hulu/hucola/data/cola_\", [\"train\", \"dev\", \"test\"], \"cola\"), \n",
    "    (\"hucopa\", \"hulu/hucopa/data/\", [\"train\", \"val\", \"test\"], \"rte\"), \n",
    "    (\"hucb\", \"hulu/hucb/data/hucb_\", [\"train\", \"dev\", \"test\"], \"rte\"), \n",
    "    (\"hurte\", \"hulu/hurte/data/rte_\", [\"train\", \"dev\", \"test\"], \"rte\"), \n",
    "    (\"husst\", \"hulu/husst/data/sst_\", [\"train\", \"dev\", \"test\"], \"sst2\"), \n",
    "    (\"huwnli\", \"hulu/huwnli/data/\", [\"train\", \"dev\", \"test\"], \"wnli\")\n",
    "]\n",
    "task = \"hucb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b34461-67d4-41f1-a9cd-fb92227a3454",
   "metadata": {},
   "source": [
    "**Note**: I had a little problem reading in '''hucb''' json files (deserialization errors), and the solution was to open them in editor, and save back with UTF-8 marker chars in the beginning of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c317c8df-c8f6-4bba-95aa-3eb017f02517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.75}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'premise', 'hypothesis', 'label'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "for (actual_task, path, variants, glue_metric) in HULU_TASKS:\n",
    "    if actual_task == task:\n",
    "        dataset = load_dataset('json', data_files=f\"{path}{variants[0]}.json\")\n",
    "        metric = load_metric('glue', glue_metric)\n",
    "        break\n",
    "\n",
    "references = [0, 1, 0, 1]\n",
    "predictions = [0, 1, 1, 1]\n",
    "results = metric.compute(predictions=predictions, references=references)\n",
    "print(results)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ac3b9-033a-4929-bdb6-be28ca70dfdb",
   "metadata": {},
   "source": [
    "## Output values\n",
    "\n",
    "The output of the metric depends on the GLUE subset chosen, consisting of a dictionary that contains one or several of the following metrics:\n",
    "\n",
    "`accuracy`: the proportion of correct predictions among the total number of cases processed, with a range between 0 and 1 (see [accuracy](https://huggingface.co/metrics/accuracy) for more information).\n",
    "\n",
    "`f1`: the harmonic mean of the precision and recall (see [F1 score](https://huggingface.co/metrics/f1) for more information). Its range is 0-1 – its lowest possible value is 0, if either the precision or the recall is 0, and its highest possible value is 1.0, which means perfect precision and recall.\n",
    "\n",
    "`pearson`: a measure of the linear relationship between two datasets (see [Pearson correlation](https://huggingface.co/metrics/pearsonr) for more information). Its range is between -1 and +1, with 0 implying no correlation, and -1/+1 implying an exact linear relationship. Positive correlations imply that as x increases, so does y, whereas negative correlations imply that as x increases, y decreases.\n",
    "\n",
    "`spearmanr`: a nonparametric measure of the monotonicity of the relationship between two datasets(see [Spearman Correlation](https://huggingface.co/metrics/spearmanr) for more information). spearmanr has the same range as pearson.\n",
    "\n",
    "`matthews_correlation`: a measure of the quality of binary and multiclass classifications (see [Matthews Correlation](https://huggingface.co/metrics/matthews_correlation) for more information). Its range of values is between -1 and +1, where a coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction.\n",
    "\n",
    "The cola subset returns matthews_correlation, the stsb subset returns pearson and spearmanr, the mrpc and qqp subsets return both accuracy and f1, and all other subsets of GLUE return only accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d3d51fa5-850d-4e5a-9867-d03654a09a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: f\"{typ.names[i]} ({i})\")\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7aca74-3887-4565-9d3b-a5081df6897f",
   "metadata": {},
   "source": [
    "We can visualize a random portion of the loaded dataset by calling the above defined method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "52f5ebcc-d93a-49d9-9bdf-95389cbe35c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>Lehet vitatkozni, hogy esetleg kicsit több volt, mint indokolt elmondani vagy sem, de mindenki hallgassa meg azokat a személyes tragédiákat, hiszen azért itt a statisztikák mögött, személyes tragédiák állnak és olyan személyek, fiatal lányok, fiatal egyetemisták esetében szabadságkorlátozó intézkedések, akik életükben hatósággal nem találkoztak. Műsorvezető: - Tehát azt mondja, hogy ezekben az ügyekben a bíróságoknak lenne tennivalójuk, valamilyen belső vizsgálat, belső kutatást el kéne végezniük?</td>\n",
       "      <td>A műsorvezető szerint azokban az ügyekben a bíróságoknak lenne tennivalójuk, valamilyen belső vizsgálatot, belső kutatást el kéne végezniük.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>Nemes Attila: - Hát ezt mondom én is. Igen, de azért nem a marketing felől jön az ötlet, hogy mit kellene bemutatni, és azt se felejtsük el hogy ott van eleve egy nagy gyűjtemény.</td>\n",
       "      <td>Nemes Attila szerint ott van eleve egy nagy gyűjtemény.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65</td>\n",
       "      <td>Nincsen alaposabb vizsgálat.  - Igen, biztos, hogy előfordul ilyen, kétségtelen, hogy nagy körültekintést igényel, de nem hiszem, hogy ennyire meg kéne ezt szigorítani, eddig is kikértük a szakorvosnak a véleményét, én nem hiszem, hogy ennek az általánossá tétele, egyértelműen hasznos lenne akár a közúti forgalom vagy a betegek részére. A cukorbetegséggel és a szívbetegséggel kapcsolatosan, nem hiszem, hogy feltétlen szakorvoshoz kellene küldeni a beteget, hiszen jól ismerjük a beteg állapotát. Nem hiszem, hogy a szemészeten és az EKG-n kívül kellene csinálni más vizsgálatot, nem tartom valószínűnek, legalábbis olyan értelemben nem, hogy nem hiszem, hogy olyan derül ki belőle, ami az illető vezetési képességét közlekedési képességét befolyásolná.</td>\n",
       "      <td>A beszélő szerint szemészeten és az EKG-n kívül kellene csinálni más vizsgálatot is.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>239</td>\n",
       "      <td>Ferenc: Szóval nem tudom mire alapozod a MIÉP szakértetlenségét, de legyen ez a te titkod.Úgy érzem túlzott ellenszenvvel viseltetsz a miéppel szemben, és ezért nem gondolod eléggé át amit írsz. Ugyanis ha a MIÉP, tegyük fel hatalomra kerülne, akkor a különböző nemzetközi bankszakemberek - tetszik-nemtetszik - az ő szakembereikkel kellene, hogy tárgyaljanak</td>\n",
       "      <td>Ferenc szerint a MIÉP hatalomra kerül.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>Úgy látszik hogy a jelenlegi városvezetés környezetében nagyon kevés olyan ember összpontosult, aki ezt a tantárgyat tanulta és érti. Így aztán előfordulhat hogy egymással párhuzamos főutakat egyidejűleg újítanak fel amik egymás közlekedési alternatívái.</td>\n",
       "      <td>A beszélő szerint egymással párhuzamos főutakat egyidejűleg újítanak fel, amik egymás közlekedési alternatívái.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>247</td>\n",
       "      <td>Asszonyszíveken különösen ninc s Bramah-závárJól van – szólt Iván. – Tehát tegyük fel, hogy sikerülend a herceget is, meg a grófot is rábírni, hogy a birtokot eladják; még akkor mindig nincsen nagyszerű établissement-od.</td>\n",
       "      <td>A beszélő szerint rábírja a birtok eladására a grófot.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>149</td>\n",
       "      <td>Gabriella: - A világ ember nélkül. Egy csodálatos fotóalbum a National Geographic kiadásában. Egyed László: - Nagyon sok oka lehet annak, ha az ember nem tesz feljelentést egy ellene elkövetett bűncselekmény miatt. Lehet ez félelem, szégyen, vagy az hogy valaki nem bízik abban, hogy bármi féle eredménye lesz a feljelentésnek, vagy megkerül a bűnös.</td>\n",
       "      <td>A beszélő szerint lesz eredménye a feljelentésnek.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35</td>\n",
       "      <td>Amikor márkanevet választunk és amikor remélhetőleg nemcsak a magyar piacra készülő márkanevet választunk, akkor figyelembe kell venni az európai sajátosságokat is. Tehát nem tudjuk sajnos úgy megjeleníteni azt, hogy magyar és a miénk és büszkék vagyunk rá, amikor Franciarországba kívánjuk ezt értékesíteni. Bordás Ákos: - Nem akarom ragozni ezt a témát, de nyilván azért vannak olyan magyar szavak is, amelyek esetleg a külföldiek számára is jól hangzó. Nem gondoltak arra, hogy esetleg megpályáztatják a márkanevet?</td>\n",
       "      <td>A beszélő szerint Bordás Ákosék megpályáztatják a márkanevet.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>167</td>\n",
       "      <td>Torba Tamás, közgazdász: Még nem láttam egyetlen politikust se kijönni tanácskozásról, aki a kezében tartotta volna a Maastrichti Egyezmény egy példányát, és látványosan kettétépte volna, tehát ilyen mese még nem történt. Az, hogy létrehoznak majd egy 120 milliárd eurós növekedési alapot, most egész Európára nézve, vagy akár csak az éppen bajban lévő mediterrán országokra nézve, ez a pénz inkább szimbolitikus jelentőségű. Műsorvezető: - Tehát úgy érti, hogy semmire sem elég?</td>\n",
       "      <td>A műsorvezető szerint a 120 milliárd eurós növekedési alap semmire sem elég.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>94</td>\n",
       "      <td>Gyurcsány Ferenc miniszterelnök: - Attól tartok, hogy Ön félreérti ezt, vagy azok a szakértők, akikre esetleg gondol vagy hivatkozik. Az IMF-hitel semmi mást nem csinál, mint az ország nemzetközi tartalékait fölemelte, és fölemeli, és azt a lehetőséget biztosítja a mi számunkra, hogy nem kell a piacon esetleg rossz feltételekkel felvenni pénzt. Ennek a reformokhoz nincsen köze. De menjünk vissza az alapkérdéshez! Én úgy látom, hogy van egy görcsös félelem bennünk, az országban, a választókban.</td>\n",
       "      <td>Gyurcsány Ferenc szerint van egy görcsös félelem a választópolgárokban.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3197b46-9236-4d21-b574-57c3304b04db",
   "metadata": {},
   "source": [
    "## Task description\n",
    "Here we define a task description for each of the tasks:\n",
    "- Where the data can be found in the dataset\n",
    "- What needs the LLM do with those\n",
    "\n",
    "Out of these a prompt will be constructed for each of the tasks, automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1aff3bc6-ffc2-4caf-ab90-c966e89eb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"hucola\": (\"Sent\", None, None, None, \"Állapítsd meg, hogy a mondat nyelvtanilag helyes (1), vagy helytelen (0).\", r'[01]'),\n",
    "    \"hucopa\": (\"premise\", \"choice1\", \"choice2\", \"question\", \"Válaszd ki, melyik mondat (1 vagy 2) a kérdésnek megfelelő válasz.\", r'[12]'),\n",
    "    \"hucb\": (\"premise\", \"hypothesis\", None, None, \"Állapítsd meg, hogy az feltételezés és felvetés milyen kapcsolatban állnak: ellentmondás (0), semleges (1), következmény (2)\", r'[012]'),\n",
    "    \"hurte\": (\"sentence1\", \"sentence2\", None, None, \"Determine if a sentence entails a given hypothesis (0) or not (1).\", r'[01]'),\n",
    "    \"husst\": (\"sentence\", None, None, None, \"Determine if the sentence has a positive (1) or negative (0) sentiment.\", r'[01]'),\n",
    "    \"huwnli\": (\"sentence1\", \"sentence2\", None, None, \"Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed (1) or not (0).\", r'[01]'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a981d21-ab3c-4c38-a547-6ce5c24ffb4a",
   "metadata": {},
   "source": [
    "## The prompt\n",
    "To prepare the prompt, we provide a system description, comming from `task_to_keys`, and additional 5 examples from the training dataset, with scores included.\n",
    "\n",
    "The actual test is comming from the \"validation\" dataset part, one by one, only providing the sentences to operate on, and simply prompting for a numerical answer by placing <|assistant|> at the line start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "71e4b095-ba36-4fc5-b5e9-99d67f55fbec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def find_first_number(input_str, matchstr):\n",
    "    match = re.search(matchstr, input_str)\n",
    "    return float(match.group()) if match else None\n",
    "\n",
    "def measure(actual_task, path, variants, glue_metric):\n",
    "    (field1, field2, field3, field4, system_prompt, matchstr) = task_to_keys[actual_task]\n",
    "    dataset = load_dataset('json', data_files=f\"{path}{variants[0]}.json\")\n",
    "    metric = load_metric('glue', glue_metric)\n",
    "    \n",
    "    prompt = f\"<|system|>{system_prompt}\\n\"\n",
    "    training_data = dataset['train']\n",
    "    \n",
    "    # Here we collect a diverse labled training set of 5 to make a few shot examples\n",
    "    last_label = 0\n",
    "    num_examples = 0\n",
    "    for idx, f1 in enumerate(training_data[field1]):\n",
    "        if training_data['label'][idx] == last_label: continue\n",
    "        prompt += f\"<|{field1 if field1 != \"Sent\" else \"sentence\"}|>{f1}\\n\"\n",
    "        if not field2 is None:\n",
    "            prompt += f\"<|{field2}|>{training_data[field2][idx]}\\n\"\n",
    "        if not field3 is None:\n",
    "            prompt += f\"<|{field3}|>{training_data[field3][idx]}\\n\"\n",
    "        if not field4 is None:\n",
    "            if training_data[field4][idx] == \"cause\":\n",
    "                prompt += f\"<|{field4}|>indok\\n\"\n",
    "            else:\n",
    "                prompt += f\"<|{field4}|>következmény\\n\"\n",
    "        prompt += f\"<|assistant|>{training_data['label'][idx]}\\n\"\n",
    "        \n",
    "        last_label = training_data['label'][idx]\n",
    "        num_examples += 1\n",
    "        if num_examples >= 5: break\n",
    "    \n",
    "    api_url = \"http://localhost:5001/api/v1\"\n",
    "    stop_words = [\"###\",\"**Observation**\",\"</s>\",\"<|\"]\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    references = []\n",
    "    predictions = []\n",
    "    failed = []\n",
    "    testset = load_dataset('json', data_files=f\"{path}{variants[1]}.json\")\n",
    "    testset = testset['train']\n",
    "    count = 0\n",
    "    for idx, f1 in enumerate(testset[field1]):\n",
    "        query = prompt\n",
    "        query += f\"<|{field1 if field1 != \"Sent\" else \"sentence\"}|>{f1}\\n\"\n",
    "        if not field2 is None:\n",
    "            query += f\"<|{field2}|>{training_data[field2][idx]}\\n\"\n",
    "        if not field3 is None:\n",
    "            query += f\"<|{field3}|>{training_data[field3][idx]}\\n\"\n",
    "        if not field4 is None:\n",
    "            if training_data[field4][idx] == \"cause\":\n",
    "                query += f\"<|{field4}|>indok\\n\"\n",
    "            else:\n",
    "                query += f\"<|{field4}|>következmény\\n\"\n",
    "        query += f\"<|assistant|>\"\n",
    "        \n",
    "        data = {\n",
    "            \"prompt\": query,\n",
    "            \"max_tokens\": 10,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"n\": 20,\n",
    "            \"stop\": stop_words\n",
    "        }\n",
    "        \n",
    "        response = requests.post(f\"{api_url}/completion\", headers=headers, json=data)\n",
    "        result = response.json()[\"choices\"][0][\"text\"]\n",
    "        for sw in stop_words:\n",
    "            result = result.replace(sw, \"\")\n",
    "        result = result.strip()\n",
    "        num_result = find_first_number(result, matchstr)\n",
    "        if num_result is None:\n",
    "            failed.append(idx)\n",
    "        else:\n",
    "            references.append(testset['label'][idx])\n",
    "            predictions.append(num_result)\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print(f\"Task: {actual_task}, {count/len(testset[field1])*100:5.1f}%\", end=\"\\r\")\n",
    "    \n",
    "    results = metric.compute(predictions=predictions, references=references)\n",
    "    return (len(testset[field1]), len(failed), results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bbbd4271-8728-4f81-bb49-e07f5d2adf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: hucopa 100/0               \n",
      "{'accuracy': 0.47}\n"
     ]
    }
   ],
   "source": [
    "for (actual_task, path, variants, glue_metric) in HULU_TASKS:\n",
    "    if actual_task == task:\n",
    "        (number, failed, result) = measure(task, path, variants, glue_metric)\n",
    "        break\n",
    "\n",
    "print(f\"Task: {task} {number}/{failed}               \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35765694-f85f-4a57-a67c-2b9802a9b157",
   "metadata": {},
   "source": [
    "# Putting all together\n",
    "- iterate through all tasks\n",
    "- load related dataset & metric\n",
    "- evaluate all verification elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f26a84fb-03f4-4133-aefa-2bede06d86b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: cola 1043/0               \n",
      "{'matthews_correlation': 0.3750517206709971}\n",
      "Task: mnli 9815/0               \n",
      "{'accuracy': 0.5387671930718289}\n",
      "Task: mrpc 408/0               \n",
      "{'accuracy': 0.7303921568627451, 'f1': 0.8270440251572327}\n",
      "Task: qnli 5463/0               \n",
      "{'accuracy': 0.557386051619989}\n",
      "Task: qqp 40430/0               \n",
      "{'accuracy': 0.6087064061340589, 'f1': 0.6251895375284306}\n",
      "Task: rte 277/0               \n",
      "{'accuracy': 0.4259927797833935}\n",
      "Task: sst2 872/0               \n",
      "{'accuracy': 0.9323394495412844}\n",
      "Task: stsb 1500/0               \n",
      "{'pearson': 0.5736636199745533, 'spearmanr': 0.5906536164846766}\n",
      "Task: wnli 71/0               \n",
      "{'accuracy': 0.5774647887323944}\n"
     ]
    }
   ],
   "source": [
    "for (task, path, variants, glue_metric) in HULU_TASKS:\n",
    "    (number, failed, result) = measure(task, path, variants, glue_metric)\n",
    "    print(f\"Task: {task} {number}/{failed}               \")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf759954-8158-4d50-abf9-d86fb561f4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
