{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b605c15-fe38-4905-96d7-6fc9301966ba",
   "metadata": {},
   "source": [
    "# HULU eval of Koboldcpp hosted LLM\n",
    "Install required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192ee022-1506-4633-8d58-e5ae2a306d39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f28fe92-34a5-4ccd-83d9-bbe414917fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install deepeval datasets evaluate requests scipy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f329a8a-934e-4916-9b7f-d7231f85e5e0",
   "metadata": {},
   "source": [
    "Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1f66cb71-794e-490a-b64a-9adb67ea03ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40349319-3430-4f9c-93f8-d2b6c3968c6c",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to evaluate one of the [Transformers](https://github.com/huggingface/transformers) model on [HuLU Benchmark](https://hulu.nytud.hu/) dataset.\n",
    "\n",
    "The HuLU Benchmark is a group of six classification tasks on sentences or pairs of sentences which are:\n",
    "\n",
    "- [HuCoLA](https://github.com/nytud/HuCOLA) (Hungarian Corpus of Linguistic Acceptability) contains 9 076 Hungarian sentences labeled for their acceptability/grammaticality (0/1).\n",
    "- [HuCoPA](https://github.com/nytud/HuCoPA) (Hungarian Choice of Plausible Alternatives Corpus) contains 1,000 instances. Each instance is composed of a premise and two alternatives. The task is to select the alternative that describes a situation standing in causal relation to the situation described by the premise.\n",
    "- [HuCB](https://github.com/nytud/HuCommitmentBank) The HuCommitmentBank consists of short text fragments in which at least one sentence contains a subordinating clause, which is syntactically subordinated to a logical inference-cancelling operator.\n",
    "- [HuRTE](https://github.com/nytud/HuRTE) (Hungarian Recognizing Textual Entailment) The dataset contains 4 504 instances. Each example contains a (sometimes multi-sentence) premise and a one-sentence hypothesis, and the task is to decide whether the former entails the latter or not.Determine if a sentence entails a given hypothesis or not.\n",
    "- [HuSST](https://github.com/nytud/HuSST) (Hungarian version of the Stanford Sentiment Treebank) contains 11 683 sentences. Each sentence is annotated for its sentiment on a three-point scale.\n",
    "- [HuWNLI](https://github.com/nytud/HuWNLI) (Winograd Natural Language Inference) Anaphora resolution datasets for Hungarian as an inference task; this is a Hungarian dataset of anaphora resolution, designed as a sentence pair classification task of natural language inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeeb854-c914-443e-8d0d-becdd3d6079c",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ec648-694f-4662-820e-43fb6f272f1d",
   "metadata": {},
   "source": [
    "We will use git clone to download data, and [Datasets](https://github.com/huggingface/datasets) library to load the data and [Evaluate](https://github.com/huggingface/evaluate) library to get the metric we need to use for evaluation (to compare our model to the benchmark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce30e634-e1f1-40a0-aeef-be8fae7ed285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'hulu/hucola'...\n",
      "remote: Enumerating objects: 136, done.\u001b[K\n",
      "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
      "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
      "remote: Total 136 (delta 9), reused 0 (delta 0), pack-reused 116 (from 1)\u001b[K\n",
      "Receiving objects: 100% (136/136), 719.81 KiB | 5.22 MiB/s, done.\n",
      "Resolving deltas: 100% (62/62), done.\n",
      "Cloning into 'hulu/hucopa'...\n",
      "remote: Enumerating objects: 98, done.\u001b[K\n",
      "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
      "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
      "remote: Total 98 (delta 42), reused 12 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (98/98), 234.20 KiB | 3.16 MiB/s, done.\n",
      "Resolving deltas: 100% (42/42), done.\n",
      "Cloning into 'hulu/hucb'...\n",
      "remote: Enumerating objects: 43, done.\u001b[K\n",
      "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
      "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
      "remote: Total 43 (delta 18), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (43/43), 147.36 KiB | 1.94 MiB/s, done.\n",
      "Resolving deltas: 100% (18/18), done.\n",
      "Cloning into 'hulu/hurte'...\n",
      "remote: Enumerating objects: 42, done.\u001b[K\n",
      "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
      "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
      "remote: Total 42 (delta 17), reused 6 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (42/42), 670.97 KiB | 5.08 MiB/s, done.\n",
      "Resolving deltas: 100% (17/17), done.\n",
      "Cloning into 'hulu/husst'...\n",
      "remote: Enumerating objects: 81, done.\u001b[K\n",
      "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
      "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
      "remote: Total 81 (delta 32), reused 18 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (81/81), 1.22 MiB | 7.88 MiB/s, done.\n",
      "Resolving deltas: 100% (32/32), done.\n",
      "Cloning into 'hulu/huwnli'...\n",
      "remote: Enumerating objects: 136, done.\u001b[K\n",
      "remote: Counting objects: 100% (131/131), done.\u001b[K\n",
      "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
      "remote: Total 136 (delta 61), reused 122 (delta 56), pack-reused 5 (from 1)\u001b[K\n",
      "Receiving objects: 100% (136/136), 154.37 KiB | 1.95 MiB/s, done.\n",
      "Resolving deltas: 100% (61/61), done.\n"
     ]
    }
   ],
   "source": [
    "!mkdir hulu\n",
    "!git clone https://github.com/nytud/HuCOLA/ hulu/hucola\n",
    "!git clone https://github.com/nytud/HuCoPA/ hulu/hucopa\n",
    "!git clone https://github.com/nytud/HuCommitmentBank/ hulu/hucb\n",
    "!git clone https://github.com/nytud/HuRTE/ hulu/hurte\n",
    "!git clone https://github.com/nytud/HuSST/ hulu/husst\n",
    "!git clone https://github.com/nytud/HuWNLI/ hulu/huwnli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e7593-23f1-429f-9d52-39d11c9a2b48",
   "metadata": {},
   "source": [
    "### Define tasks\n",
    "What can be found where, and which metric belongs to which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b45377c7-7f69-4c11-90ff-b99a3f9e907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HULU_TASKS = [\n",
    "    (\"hucola\", \"hulu/hucola/data/cola_\", [\"train\", \"dev\", \"test\"], \"cola\"), \n",
    "    (\"hucopa\", \"hulu/hucopa/data/\", [\"train\", \"val\", \"test\"], \"rte\"), \n",
    "    (\"hucb\", \"hulu/hucb/data/hucb_\", [\"train\", \"dev\", \"test\"], \"mnli\"), \n",
    "    (\"hurte\", \"hulu/hurte/data/rte_\", [\"train\", \"dev\", \"test\"], \"rte\"), \n",
    "    (\"husst\", \"hulu/husst/data/sst_\", [\"train\", \"dev\", \"test\"], \"sst2\"), \n",
    "    (\"huwnli\", \"hulu/huwnli/data/\", [\"train\", \"dev\", \"test\"], \"wnli\")\n",
    "]\n",
    "task = \"hucb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b34461-67d4-41f1-a9cd-fb92227a3454",
   "metadata": {},
   "source": [
    "**Note**: I had a little problem reading in '''hucb''' json files (deserialization errors), and the solution was to open them in editor, and save back with UTF-8 marker chars in the beginning of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c317c8df-c8f6-4bba-95aa-3eb017f02517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.75}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'premise', 'hypothesis', 'label'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "for (actual_task, path, variants, glue_metric) in HULU_TASKS:\n",
    "    if actual_task == task:\n",
    "        dataset = load_dataset('json', data_files=f\"{path}{variants[0]}.json\")\n",
    "        metric = load_metric('glue', glue_metric)\n",
    "        break\n",
    "\n",
    "references = [0, 1, 0, 1]\n",
    "predictions = [0, 1, 1, 1]\n",
    "results = metric.compute(predictions=predictions, references=references)\n",
    "print(results)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ac3b9-033a-4929-bdb6-be28ca70dfdb",
   "metadata": {},
   "source": [
    "## Output values\n",
    "\n",
    "The output of the metric depends on the GLUE subset chosen, consisting of a dictionary that contains one or several of the following metrics:\n",
    "\n",
    "`accuracy`: the proportion of correct predictions among the total number of cases processed, with a range between 0 and 1 (see [accuracy](https://huggingface.co/metrics/accuracy) for more information).\n",
    "\n",
    "`f1`: the harmonic mean of the precision and recall (see [F1 score](https://huggingface.co/metrics/f1) for more information). Its range is 0-1 – its lowest possible value is 0, if either the precision or the recall is 0, and its highest possible value is 1.0, which means perfect precision and recall.\n",
    "\n",
    "`pearson`: a measure of the linear relationship between two datasets (see [Pearson correlation](https://huggingface.co/metrics/pearsonr) for more information). Its range is between -1 and +1, with 0 implying no correlation, and -1/+1 implying an exact linear relationship. Positive correlations imply that as x increases, so does y, whereas negative correlations imply that as x increases, y decreases.\n",
    "\n",
    "`spearmanr`: a nonparametric measure of the monotonicity of the relationship between two datasets(see [Spearman Correlation](https://huggingface.co/metrics/spearmanr) for more information). spearmanr has the same range as pearson.\n",
    "\n",
    "`matthews_correlation`: a measure of the quality of binary and multiclass classifications (see [Matthews Correlation](https://huggingface.co/metrics/matthews_correlation) for more information). Its range of values is between -1 and +1, where a coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction.\n",
    "\n",
    "The cola subset returns matthews_correlation, the stsb subset returns pearson and spearmanr, the mrpc and qqp subsets return both accuracy and f1, and all other subsets of GLUE return only accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d3d51fa5-850d-4e5a-9867-d03654a09a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: f\"{typ.names[i]} ({i})\")\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7aca74-3887-4565-9d3b-a5081df6897f",
   "metadata": {},
   "source": [
    "We can visualize a random portion of the loaded dataset by calling the above defined method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "52f5ebcc-d93a-49d9-9bdf-95389cbe35c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206</td>\n",
       "      <td>Pázmány Péter, Dunaszerdahely polgármestere: - Abban a szektorban, ahol a támadás történt, mintegy 2-300 ember volt, ott valóban volt Nagy-Magyarország zászló is kifeszítve. Viszont ami maga az eseményeket illeti, úgy láttam, hiszen a helyszínen voltam, hogy semmilyen provokációt - tehát semmilyen kődobálás és a rendőrök megtámadása nem történt meg, hanem a mérkőzés 17. percében én személy szerint és a körülöttem lévő szurkolók érthetetlenül láttuk, hogy a rendőrség benyomul ebbe a szektorba. Műsorvezető: - És ön is azt mondja, hogy ennek semmiféle előzetes jele nem volt, nem volt figyelmeztetés például?</td>\n",
       "      <td>A műsorvezető szerint a támadásnak semmiféle előzetes jele nem volt.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>226</td>\n",
       "      <td>Ladányi János szociológus: - Hát először is azt szeretném mondani, hogy nincs más választásunk, mint hogy el kell kezdenünk normális diskurzust folytatni erről a dologról. Vagyis normális diskuzusokat. És nemcsak a politikától kell várni azt, hogy ez a normális diskurzus megkezdődjön, hanem mondjuk a médiában is le kellene folytatni ezt a normális diskurzust, mert hogyha hetek óta és hónapok óta, hogyha kinyitok mondok bizonyos televíziós csatornákat, és az első négy hír az romákkal kapcsolatos bűnesetekről szól, akkor mindenkiben az a kép rajzolódik ki, ami kirajzolódik, és hát ez nagyon kevésbé realisztikus. Műsorvezető: - Tehát azt mondja, hogy ez nem igaz, ez nem a valóságot ábrázolja?</td>\n",
       "      <td>A műsorvezető szerint ez a kép nem a valóságot ábrázolja.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>172</td>\n",
       "      <td>Oszkó Péter: - A Pénzügyminisztérium, bocsánat a volt pénzügyminisztérium épületébe, tehát a József Nádor térre. Műsorvezető: - És pontos időpontja van, hogy mikor kell Matolcsy Györggyel találkozni, vagy előszobázik? Oszkó Péter: - Egészen tegnap délig úgy tudtam, hogy ez ma egykor lesz ez az átadás-átvétel, aztán hirtelen telefonok tömkelegét kaptam, miszerint még nem biztos, hogy lehet, hogy délelőtt tízkor.</td>\n",
       "      <td>Oszkó Péter szerint aznap egykor van az átadás-átvétel.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>146</td>\n",
       "      <td>A legtöbb cég úgy találja, hogy például Guatemalában és Zambiában, Belaruszban, az üzleti élet szabályozói teljesen kiszámíthatatlanok. A vállalatok több mint hetven százaléka Bangladesben, Ecuadorban és Moldovában nem bízik abban, hogy a helyi bíróságok megerősítenék tulajdonjogukat.</td>\n",
       "      <td>A beszélő szerint a helyi bíróságok megerősítenék tulajdonjogukat.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>Csúszik a Margithíd felújítása. A Távirati Iroda úgy tudja, hogy május helyett, csak nyáron kezdődik el a budapesti híd rekonstrukciója, mert három hónapot kellett várni egy hatósági engedélyre.</td>\n",
       "      <td>A beszélő szerint május helyett csak nyáron kezdődik el a budapesti híd rekonstrukciója, mert három hónapot kellett várni egy hatósági engedélyre.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>77</td>\n",
       "      <td>Nincs ebben semmi meglepő. - mondja a kormányszóvivő. Szollár Domonkos szerint, a kabinet nem a cég nagysága szerint dönt a támogatásokról. A mai kormányülésen várhatóan szóba kerül, hogy hat-hét milliárd forintot fizetnének az OmnInvestnek a 6 millió oltóanyagért. Szollár Domonkos: - A cél az, hogy legyen oltóanyag, ha ezt az oltóanyagot, egy bármilyen kiscég gyártja, ha előbb tudja gyártani, meg tudja találni a megfelelő oltóanyagot, akkor azt meg kell vásárolni a kiscégtől, nem hiszem, hogy itt a kiscég mérete lenne a döntő, a cél az, hogy legyen oltóanyag és a jelen állás szerint, úgy néz ki, hogy Magyarországon, a világon az első között lesz olyan oltóanyag, amelyet lehet használni ez ellen a vírus ellen.</td>\n",
       "      <td>SZollár Domonkos szerint a cég mérete a döntő az oltóanyaggyártásnál.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>137</td>\n",
       "      <td>Riporter: - Nem olvasnak eleget a mai fiatalok és mindenki a számítógépen bele-belelapoz valamibe. Amikor beléptem a Szabó Ervin könyvtárba borzasztó tömeg volt. Tudom, hogy mindig tömeg van. Tehát, ha valaki ebben él, akkor azt hiszi, hogy mindenki olvas?</td>\n",
       "      <td>A beszélő szerint mindenki olvas.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>244</td>\n",
       "      <td>Majd így okoskodék hősünk tovább:»Talán beavassam élettörténetembe, elmondjak szögről-végre mindent, anyámról, atyámról, halálozási körülményeiről? Tegyük fel, elhiszi alfától omegáig: mit ér az?</td>\n",
       "      <td>A beszélő szerint elmondják neki az élettörténetüket.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>86</td>\n",
       "      <td>Zsuzsanna: - Ennek az élettartama számítások szerint 30 éven túl van, ugye a megtérülési idő után, csak karbantartást igényel, utána már csak hozza az eredményeket. Riporter: - Csak álljon még utána a ház is. Zsuzsanna: - Én attól nem félek, hogy egy panelépület nem fog állni, főleg azért nem félek tőle, mert az épület fel lett újítva, és ezzel az élettartamát 80-90-100 évre, számítások szerint 100 évig eltart.</td>\n",
       "      <td>A beszélő szerint a régi panelépület nem lesz használható.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>216</td>\n",
       "      <td>Riporter: - Hogyan lehetett volna megelőzni? Frenkl Róbert: - Ugy, hogy nem indulnak el, vagy pedig nem indul el az a versenyző, akinél ilyen probléma fellép. Riporter: - Na most ez egy gyanúsítás, azt mondja, hogy Fazekas gyanús?</td>\n",
       "      <td>A Riporter szerint Fazekas gyanús.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3197b46-9236-4d21-b574-57c3304b04db",
   "metadata": {},
   "source": [
    "## Task description\n",
    "Here we define a task description for each of the tasks:\n",
    "- Where the data can be found in the dataset\n",
    "- What needs the LLM do with those\n",
    "- Regexp for the result to parse for\n",
    "\n",
    "Out of these a prompt will be constructed for each of the tasks, automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1aff3bc6-ffc2-4caf-ab90-c966e89eb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"hucola\": (\"Sent\", None, None, None, \"Határozd meg, hogy a mondat nyelvtanilag helyes-e (1), vagy helytelen (0).\", r'[01]'),\n",
    "    \"hucopa\": (\"premise\", \"choice1\", \"choice2\", \"question\", \"Válaszd ki, melyik mondat (1 vagy 2) a kérdésnek megfelelő válasz.\", r'[12]'),\n",
    "    \"hucb\": (\"premise\", \"hypothesis\", None, None, \"Határozd meg, hogy a premisa és hipotézis milyen kapcsolatban állnak: ellentmondás (0), semleges (1), következmény (2)\", r'[012]'),\n",
    "    \"hurte\": (\"premise\", \"hypothesis\", None, None, \"Határozd meg, hogy a premisából következik-e (1) a hipotézis, vagy sem (0).\", r'[01]'),\n",
    "    \"husst\": (\"Sent\", None, None, None, \"Határozd meg, hogy a mondat pozitív (1) vagy negatív (0) hangulatú-e.\", r'[01]'),\n",
    "    \"huwnli\": (\"sentence1\", \"sentence2\", None, None, \"Határozd meg, hogy az első mondatból következik-e (1) a második mondat, vagy sem (0).\", r'[01]'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a981d21-ab3c-4c38-a547-6ce5c24ffb4a",
   "metadata": {},
   "source": [
    "## The prompt\n",
    "To prepare the prompt, we provide a system description, comming from `task_to_keys`, and additional 5 examples from the training dataset, with scores included.\n",
    "\n",
    "The actual test is comming from the \"validation\" dataset part, one by one, only providing the sentences to operate on, and simply prompting for a numerical answer by placing <|assistant|> at the line start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71e4b095-ba36-4fc5-b5e9-99d67f55fbec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def find_first_number(input_str, matchstr):\n",
    "    match = re.search(matchstr, input_str)\n",
    "    return float(match.group()) if match else None\n",
    "\n",
    "def get_label_value(input_str):\n",
    "    num = find_first_number(input_str, r'\\d+(\\.\\d+)?')\n",
    "    if not num is None:\n",
    "        return num\n",
    "    if input_str == \"contradiction\" or input_str == \"negative\":\n",
    "        return 0\n",
    "    elif input_str == \"neutral\" or input_str == \"positive\":\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "def measure(actual_task, path, variants, glue_metric):\n",
    "    (field1, field2, field3, field4, system_prompt, matchstr) = task_to_keys[actual_task]\n",
    "    dataset = load_dataset('json', data_files=f\"{path}{variants[0]}.json\")\n",
    "    metric = load_metric('glue', glue_metric)\n",
    "    \n",
    "    prompt = f\"<|system|>{system_prompt}\\n\"\n",
    "    training_data = dataset['train']\n",
    "    \n",
    "    # Here we collect a diverse labled training set of 5 to make a few shot examples\n",
    "    last_label = 0\n",
    "    num_examples = 0\n",
    "    for idx, f1 in enumerate(training_data[field1]):\n",
    "        if training_data['label'][idx] == last_label: continue\n",
    "        prompt += f\"<|{field1 if field1 != \"Sent\" else \"sentence\"}|>{f1}\\n\"\n",
    "        if not field2 is None:\n",
    "            prompt += f\"<|{field2}|>{training_data[field2][idx]}\\n\"\n",
    "        if not field3 is None:\n",
    "            prompt += f\"<|{field3}|>{training_data[field3][idx]}\\n\"\n",
    "        if not field4 is None:\n",
    "            if training_data[field4][idx] == \"cause\":\n",
    "                prompt += f\"<|{field4}|>indok\\n\"\n",
    "            else:\n",
    "                prompt += f\"<|{field4}|>következmény\\n\"\n",
    "        prompt += f\"<|assistant|>{get_label_value(training_data['label'][idx])}\\n\"\n",
    "        \n",
    "        last_label = training_data['label'][idx]\n",
    "        num_examples += 1\n",
    "        if num_examples >= 5: break\n",
    "    \n",
    "    api_url = \"http://localhost:5001/api/v1\"\n",
    "    stop_words = [\"###\",\"**Observation**\",\"</s>\",\"<|\"]\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    references = []\n",
    "    predictions = []\n",
    "    failed = []\n",
    "    testset = load_dataset('json', data_files=f\"{path}{variants[1]}.json\")\n",
    "    testset = testset['train']\n",
    "    count = 0\n",
    "    for idx, f1 in enumerate(testset[field1]):\n",
    "        query = prompt\n",
    "        query += f\"<|{field1 if field1 != \"Sent\" else \"sentence\"}|>{f1}\\n\"\n",
    "        if not field2 is None:\n",
    "            query += f\"<|{field2}|>{training_data[field2][idx]}\\n\"\n",
    "        if not field3 is None:\n",
    "            query += f\"<|{field3}|>{training_data[field3][idx]}\\n\"\n",
    "        if not field4 is None:\n",
    "            if training_data[field4][idx] == \"cause\":\n",
    "                query += f\"<|{field4}|>indok\\n\"\n",
    "            else:\n",
    "                query += f\"<|{field4}|>következmény\\n\"\n",
    "        query += f\"<|assistant|>\"\n",
    "        \n",
    "        data = {\n",
    "            \"prompt\": query,\n",
    "            \"max_tokens\": 10,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"n\": 20,\n",
    "            \"stop\": stop_words\n",
    "        }\n",
    "        \n",
    "        response = requests.post(f\"{api_url}/completion\", headers=headers, json=data)\n",
    "        result = response.json()[\"choices\"][0][\"text\"]\n",
    "        for sw in stop_words:\n",
    "            result = result.replace(sw, \"\")\n",
    "        result = result.strip()\n",
    "        num_result = find_first_number(result, matchstr)\n",
    "        if num_result is None:\n",
    "            failed.append(idx)\n",
    "        else:\n",
    "            references.append(get_label_value(testset['label'][idx]))\n",
    "            predictions.append(num_result)\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print(f\"Task: {actual_task}, {count/len(testset[field1])*100:5.1f}%\", end=\"\\r\")\n",
    "    \n",
    "    results = metric.compute(predictions=predictions, references=references)\n",
    "    return (len(testset[field1]), len(failed), results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bbbd4271-8728-4f81-bb49-e07f5d2adf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: huwnli 60/0               \n",
      "{'accuracy': 0.55}\n"
     ]
    }
   ],
   "source": [
    "for (actual_task, path, variants, glue_metric) in HULU_TASKS:\n",
    "    if actual_task == task:\n",
    "        (number, failed, result) = measure(task, path, variants, glue_metric)\n",
    "        break\n",
    "\n",
    "print(f\"Task: {task} {number}/{failed}               \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35765694-f85f-4a57-a67c-2b9802a9b157",
   "metadata": {},
   "source": [
    "# Putting all together\n",
    "- iterate through all tasks\n",
    "- load related dataset & metric\n",
    "- evaluate all verification elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f26a84fb-03f4-4133-aefa-2bede06d86b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: hucola 910/0               \n",
      "{'matthews_correlation': 0.07021490563070865}\n",
      "Task: hucopa 100/0               \n",
      "{'accuracy': 0.49}\n",
      "Task: hucb 103/2               \n",
      "{'accuracy': 0.39603960396039606}\n",
      "Task: hurte 243/0               \n",
      "{'accuracy': 0.4609053497942387}\n",
      "Task: husst 1165/0               \n",
      "{'accuracy': 0.6738197424892703}\n",
      "Task: huwnli 60/0               \n",
      "{'accuracy': 0.4666666666666667}\n"
     ]
    }
   ],
   "source": [
    "for (task, path, variants, glue_metric) in HULU_TASKS:\n",
    "    (number, failed, result) = measure(task, path, variants, glue_metric)\n",
    "    print(f\"Task: {task} {number}/{failed}               \")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf759954-8158-4d50-abf9-d86fb561f4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
