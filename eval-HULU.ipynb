{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b605c15-fe38-4905-96d7-6fc9301966ba",
   "metadata": {},
   "source": [
    "# HuLU eval of Koboldcpp hosted LLM\n",
    "HuLU dataset is created with similar goals in mind as GLUE, but for Hungarian specific language evaluation in mind. This means, it is just a classification task list, which required to be evaluated on any kind of NLP classifier you can throw at it.\n",
    "\n",
    "It's intended workflow is the following:\n",
    "- One trains the classifier with training set from the whole dataset\n",
    "- Checks back with the evaluation dataset how it performs (it is also labeled)\n",
    "- Then runs the whole Test set, and stores the results (it doesn't contain labels, it's private)\n",
    "- Uploads the result to somewhere, which calculates the metrics, and possibly displays it's score on a leader board\n",
    "\n",
    "## Why keep the test labels secret?\n",
    "Simple enough: to prevent the LLMs to contaminate their knowledge with the correct results, thus preventing some LLM developers to artificially train their LLMs with the good results, thus advancing their creation on the leader board.\n",
    "\n",
    "Though, so far I didn't came accross any leader boards, nor services which calculates the metrics for me.\n",
    "\n",
    "## How to evaluate then?\n",
    "The idea, is to use the evaluation part of the dataset, to measure. We skip the part, where we train the LLM for the classification part, and instead we use the context learning capability of the model: we give a short description of the work they need to perform, and give some examples (5 diverse examples to be concrate) from the training set. This gives us a few-shot-example-prompt, which according to my experience, most of the LLMs can easily comply. For this, to work, the most interresting candidates for evaluation, are the instruction finetuned versions of the models.\n",
    "\n",
    "## Why use Kobobldcpp to host LLM?\n",
    "Koboldcpp is easy to configure through command line, can use CPU, CUDA, OpenCL, etc, so it's easy to squeeze out most of what's available on the local, or remote machine, without touching this very notebook, and it's codes. Also, one can use any arbitrary quantized version of any openly available LLM, provided Koboldcpp is capable to run it. It it can run, a lot of different types of LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f28fe92-34a5-4ccd-83d9-bbe414917fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./jupyter/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: evaluate in ./jupyter/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: requests in ./jupyter/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: scipy in ./jupyter/lib/python3.12/site-packages (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in ./jupyter/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: openai in ./jupyter/lib/python3.12/site-packages (1.51.2)\n",
      "Requirement already satisfied: filelock in ./jupyter/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./jupyter/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./jupyter/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./jupyter/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./jupyter/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./jupyter/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in ./jupyter/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./jupyter/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in ./jupyter/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in ./jupyter/lib/python3.12/site-packages (from datasets) (3.10.9)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in ./jupyter/lib/python3.12/site-packages (from datasets) (0.25.2)\n",
      "Requirement already satisfied: packaging in ./jupyter/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./jupyter/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./jupyter/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./jupyter/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./jupyter/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./jupyter/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./jupyter/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./jupyter/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./jupyter/lib/python3.12/site-packages (from openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./jupyter/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./jupyter/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./jupyter/lib/python3.12/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./jupyter/lib/python3.12/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in ./jupyter/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./jupyter/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./jupyter/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./jupyter/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./jupyter/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./jupyter/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./jupyter/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./jupyter/lib/python3.12/site-packages (from aiohttp->datasets) (1.14.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./jupyter/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./jupyter/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./jupyter/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./jupyter/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./jupyter/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./jupyter/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./jupyter/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./jupyter/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./jupyter/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate requests scipy scikit-learn openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f329a8a-934e-4916-9b7f-d7231f85e5e0",
   "metadata": {},
   "source": [
    "Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f66cb71-794e-490a-b64a-9adb67ea03ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40349319-3430-4f9c-93f8-d2b6c3968c6c",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to evaluate one of the [Transformers](https://github.com/huggingface/transformers) model on [HuLU Benchmark](https://hulu.nytud.hu/) dataset.\n",
    "\n",
    "The HuLU Benchmark is a group of six classification tasks on sentences or pairs of sentences which are:\n",
    "\n",
    "- [HuCoLA](https://github.com/nytud/HuCOLA) (Hungarian Corpus of Linguistic Acceptability) contains 9 076 Hungarian sentences labeled for their acceptability/grammaticality (0/1).\n",
    "- [HuCoPA](https://github.com/nytud/HuCoPA) (Hungarian Choice of Plausible Alternatives Corpus) contains 1,000 instances. Each instance is composed of a premise and two alternatives. The task is to select the alternative that describes a situation standing in causal relation to the situation described by the premise.\n",
    "- [HuCB](https://github.com/nytud/HuCommitmentBank) The HuCommitmentBank consists of short text fragments in which at least one sentence contains a subordinating clause, which is syntactically subordinated to a logical inference-cancelling operator.\n",
    "- [HuRTE](https://github.com/nytud/HuRTE) (Hungarian Recognizing Textual Entailment) The dataset contains 4 504 instances. Each example contains a (sometimes multi-sentence) premise and a one-sentence hypothesis, and the task is to decide whether the former entails the latter or not.Determine if a sentence entails a given hypothesis or not.\n",
    "- [HuSST](https://github.com/nytud/HuSST) (Hungarian version of the Stanford Sentiment Treebank) contains 11 683 sentences. Each sentence is annotated for its sentiment on a three-point scale.\n",
    "- [HuWNLI](https://github.com/nytud/HuWNLI) (Winograd Natural Language Inference) Anaphora resolution datasets for Hungarian as an inference task; this is a Hungarian dataset of anaphora resolution, designed as a sentence pair classification task of natural language inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeeb854-c914-443e-8d0d-becdd3d6079c",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ec648-694f-4662-820e-43fb6f272f1d",
   "metadata": {},
   "source": [
    "We will use git clone to download data, and [Datasets](https://github.com/huggingface/datasets) library to load the data and [Evaluate](https://github.com/huggingface/evaluate) library to get the metric we need to use for evaluation (to compare our model to the benchmark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce30e634-e1f1-40a0-aeef-be8fae7ed285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'hulu/hucola'...\n",
      "Cloning into 'hulu/hucopa'...\n",
      "Cloning into 'hulu/hucb'...\n",
      "Cloning into 'hulu/hurte'...\n",
      "Cloning into 'hulu/husst'...\n",
      "Cloning into 'hulu/huwnli'...\n"
     ]
    }
   ],
   "source": [
    "!mkdir hulu\n",
    "!git clone https://github.com/nytud/HuCOLA/ hulu/hucola\n",
    "!git clone https://github.com/nytud/HuCoPA/ hulu/hucopa\n",
    "!git clone https://github.com/nytud/HuCommitmentBank/ hulu/hucb\n",
    "!git clone https://github.com/nytud/HuRTE/ hulu/hurte\n",
    "!git clone https://github.com/nytud/HuSST/ hulu/husst\n",
    "!git clone https://github.com/nytud/HuWNLI/ hulu/huwnli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e7593-23f1-429f-9d52-39d11c9a2b48",
   "metadata": {},
   "source": [
    "### Define tasks\n",
    "What can be found where, and which metric belongs to which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45377c7-7f69-4c11-90ff-b99a3f9e907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HULU_TASKS = [\n",
    "    (\"hucola\", \"hulu/hucola/data/cola_\", [\"train\", \"dev\", \"test\"], \"cola\"), \n",
    "    (\"hucopa\", \"hulu/hucopa/data/\", [\"train\", \"val\", \"test\"], \"rte\"), \n",
    "    (\"hucb\", \"hulu/hucb/data/hucb_\", [\"train\", \"dev\", \"test\"], \"mnli\"), \n",
    "    (\"hurte\", \"hulu/hurte/data/rte_\", [\"train\", \"dev\", \"test\"], \"rte\"), \n",
    "    (\"husst\", \"hulu/husst/data/sst_\", [\"train\", \"dev\", \"test\"], \"sst2\"), \n",
    "    (\"huwnli\", \"hulu/huwnli/data/\", [\"train\", \"dev\", \"test\"], \"wnli\")\n",
    "]\n",
    "task = \"huwnli\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b34461-67d4-41f1-a9cd-fb92227a3454",
   "metadata": {},
   "source": [
    "**Note**: I had a little problem reading in '''hucb''' json files (deserialization errors), and the solution was to open them in editor, and save back with UTF-8 marker chars in the beginning of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c317c8df-c8f6-4bba-95aa-3eb017f02517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.75}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['orig_id', 'id', 'sentence1', 'sentence2', 'label', 'Column6'],\n",
       "        num_rows: 562\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "for (actual_task, path, variants, glue_metric) in HULU_TASKS:\n",
    "    if actual_task == task:\n",
    "        dataset = load_dataset('json', data_files=f\"{path}{variants[0]}.json\")\n",
    "        metric = load_metric('glue', glue_metric)\n",
    "        break\n",
    "\n",
    "references = [0, 1, 0, 1]\n",
    "predictions = [0, 1, 1, 1]\n",
    "results = metric.compute(predictions=predictions, references=references)\n",
    "print(results)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ac3b9-033a-4929-bdb6-be28ca70dfdb",
   "metadata": {},
   "source": [
    "## Output values\n",
    "\n",
    "The output of the metric depends on the GLUE subset chosen, consisting of a dictionary that contains one or several of the following metrics:\n",
    "\n",
    "`accuracy`: the proportion of correct predictions among the total number of cases processed, with a range between 0 and 1 (see [accuracy](https://huggingface.co/metrics/accuracy) for more information).\n",
    "\n",
    "`f1`: the harmonic mean of the precision and recall (see [F1 score](https://huggingface.co/metrics/f1) for more information). Its range is 0-1 – its lowest possible value is 0, if either the precision or the recall is 0, and its highest possible value is 1.0, which means perfect precision and recall.\n",
    "\n",
    "`pearson`: a measure of the linear relationship between two datasets (see [Pearson correlation](https://huggingface.co/metrics/pearsonr) for more information). Its range is between -1 and +1, with 0 implying no correlation, and -1/+1 implying an exact linear relationship. Positive correlations imply that as x increases, so does y, whereas negative correlations imply that as x increases, y decreases.\n",
    "\n",
    "`spearmanr`: a nonparametric measure of the monotonicity of the relationship between two datasets(see [Spearman Correlation](https://huggingface.co/metrics/spearmanr) for more information). spearmanr has the same range as pearson.\n",
    "\n",
    "`matthews_correlation`: a measure of the quality of binary and multiclass classifications (see [Matthews Correlation](https://huggingface.co/metrics/matthews_correlation) for more information). Its range of values is between -1 and +1, where a coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction.\n",
    "\n",
    "The cola subset returns matthews_correlation, the stsb subset returns pearson and spearmanr, the mrpc and qqp subsets return both accuracy and f1, and all other subsets of GLUE return only accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3d51fa5-850d-4e5a-9867-d03654a09a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: f\"{typ.names[i]} ({i})\")\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7aca74-3887-4565-9d3b-a5081df6897f",
   "metadata": {},
   "source": [
    "We can visualize a random portion of the loaded dataset by calling the above defined method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52f5ebcc-d93a-49d9-9bdf-95389cbe35c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_id</th>\n",
       "      <th>id</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>Column6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>411</td>\n",
       "      <td>360</td>\n",
       "      <td>Kimentem egy kis ételért, inkább az idő elütése végett, mint azért, mert szükségem volt rá.</td>\n",
       "      <td>Kimentem egy kis ételért, inkább az idő elütése végett, mint azért, mert szükségem volt az időre.</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>Betettem a tortát a hűtőbe. Sok vaj van benne.</td>\n",
       "      <td>A tortában sok vaj van.</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137</td>\n",
       "      <td>118</td>\n",
       "      <td>Nem találtam kanalat, szóval egy tollal próbáltam megkeverni a kávémat. Ez rossz ötletnek bizonyult, ugyanis tele lett tintával.</td>\n",
       "      <td>A toll tele lett tintával.</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>563</td>\n",
       "      <td>491</td>\n",
       "      <td>Az asztalra raktam a nehéz könyvet és összetört.</td>\n",
       "      <td>Az asztal összetört.</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>Laci eddig mindig segített apának a munkával. De most nem tudott segíteni neki, mert apa azt mondta, hogy a főnöke a vasúti társaságnál nem akarja, hogy rajta kívül bárki az irodájában dolgozzon.</td>\n",
       "      <td>Apa most nem tudott segíteni.</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>482</td>\n",
       "      <td>421</td>\n",
       "      <td>Alíz a nappalit porolta, és próbálta megtalálni a gombot, amit anya elrejtett. Ma nem volt ideje arra, hogy régi képeket nézegessen a kedvenc fotóalbumában. Ma egy gombot kellett keresnie, ezért az albumot egy székre tette anélkül, hogy kinyitotta volna .</td>\n",
       "      <td>Az albumot egy székre tette anélkül, hogy kinyitotta volna az albumot.</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>545</td>\n",
       "      <td>477</td>\n",
       "      <td>Sam Goodman monográfiája a spártai tábornok Xenophanészról jól tükrözi azokat a nehézségeket, amelyekkel gyerekkora során szembesült.</td>\n",
       "      <td>Xenophanész nehézségekkel szembesült.</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>573</td>\n",
       "      <td>497</td>\n",
       "      <td>Janka bekopogtatott Zsuzsához, de senki nem nyitott ajtót. Csalódott volt.</td>\n",
       "      <td>Janka csalódott volt.</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>39</td>\n",
       "      <td>34</td>\n",
       "      <td>Jakab megnyugtatta Kevint, mert zaklatott volt.</td>\n",
       "      <td>Kevin zaklatott volt.</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>517</td>\n",
       "      <td>452</td>\n",
       "      <td>Erzsi nem lett mérges Sacira, aki félbeszakította, mert megállt és bocsánatot kért.</td>\n",
       "      <td>Erzsi bocsánatot kért.</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3197b46-9236-4d21-b574-57c3304b04db",
   "metadata": {},
   "source": [
    "## Task description\n",
    "Here we define a task description for each of the tasks:\n",
    "- Where the data can be found in the dataset\n",
    "- What needs the LLM do with those\n",
    "- Regexp for the result to parse for\n",
    "\n",
    "Out of these a prompt will be constructed for each of the tasks, automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aff3bc6-ffc2-4caf-ab90-c966e89eb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"hucola\": (\"Sent\", None, None, None, \"Határozd meg, hogy a mondat nyelvtanilag helyes-e (1), vagy helytelen (0).\", r'[01]'),\n",
    "    \"hucopa\": (\"premise\", \"choice1\", \"choice2\", \"question\", \"Válaszd ki, melyik mondat (1 vagy 2) a kérdésnek megfelelő válasz.\", r'[12]'),\n",
    "    \"hucb\": (\"premise\", \"hypothesis\", None, None, \"Határozd meg, hogy a premisa és hipotézis milyen kapcsolatban állnak: ellentmondás (0), semleges (1), következmény (2)\", r'[012]'),\n",
    "    \"hurte\": (\"premise\", \"hypothesis\", None, None, \"Határozd meg, hogy a premisából következik-e (1) a hipotézis, vagy sem (0).\", r'[01]'),\n",
    "    \"husst\": (\"Sent\", None, None, None, \"Határozd meg, hogy a mondat pozitív (1) vagy negatív (0) hangulatú-e.\", r'[01]'),\n",
    "    \"huwnli\": (\"sentence1\", \"sentence2\", None, None, \"Határozd meg, hogy az első mondatból következik-e (1) a második mondat, vagy sem (0).\", r'[01]'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db75076-72f4-42dd-a79b-4a827e9af9c3",
   "metadata": {},
   "source": [
    "## Calling the LLM web service\n",
    "For this, we define a method. Only select/run the one you want to use.\n",
    "\n",
    "### 1.) Koboldcpp web service call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67dbfbc5-33d4-44a9-a460-df2525af9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def call_llm(prompt):\n",
    "    api_url = \"http://localhost:5001/api/v1\"\n",
    "    stop_words = [\"###\",\"**Observation**\",\"</s>\",\"<|\"]\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 10,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"n\": 20,\n",
    "        \"stop\": stop_words\n",
    "    }\n",
    "    \n",
    "    response = requests.post(f\"{api_url}/completion\", headers=headers, json=data)\n",
    "    result = response.json()[\"choices\"][0][\"text\"]\n",
    "    for sw in stop_words:\n",
    "        result = result.replace(sw, \"\")\n",
    "    return result.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514349e-0222-4788-9797-6c070b9043fc",
   "metadata": {},
   "source": [
    "### 2.) AzureOpenAI web service call\n",
    "For this to work, one need to define 3 environment variables:\n",
    "- ```AZURE_OPENAI_API_KEY```\n",
    "- ```AZURE_OPENAI_ENDPOINT```\n",
    "- ```AZURE_OPENAI_DEPLOYMENT_NAME```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1321c734-b7f4-4f8d-bcc7-e364ca61fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "azureClient = AzureOpenAI(api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"), azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), api_version = \"2023-05-15\")\n",
    "def call_llm(prompt):\n",
    "    try:\n",
    "        response = azureClient.completions.create(\n",
    "            model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),  # Specify the engine you want to use\n",
    "            prompt=prompt,\n",
    "            max_tokens=10,  # Adjust the number of tokens as needed\n",
    "            temperature=0\n",
    "        )\n",
    "    \n",
    "        return response.choices[0].text.strip()\n",
    "    except Exception as e:\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5165326-13fd-44e2-ae53-f859a69800b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 times 2 is 4.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_llm(\"Tell me, how much is 2x2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a981d21-ab3c-4c38-a547-6ce5c24ffb4a",
   "metadata": {},
   "source": [
    "## The prompt\n",
    "To prepare the prompt, we provide a system description, comming from `task_to_keys`, and additional 5 examples from the training dataset, with scores included.\n",
    "\n",
    "The actual test is comming from the \"validation\" dataset part, one by one, only providing the sentences to operate on, and simply prompting for a numerical answer by placing <|assistant|> at the line start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71e4b095-ba36-4fc5-b5e9-99d67f55fbec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def find_first_number(input_str, matchstr):\n",
    "    match = re.search(matchstr, input_str)\n",
    "    return float(match.group()) if match else None\n",
    "\n",
    "def get_label_value(input_str):\n",
    "    num = find_first_number(input_str, r'\\d+(\\.\\d+)?')\n",
    "    if not num is None:\n",
    "        return num\n",
    "    if input_str == \"contradiction\" or input_str == \"negative\":\n",
    "        return 0\n",
    "    elif input_str == \"neutral\" or input_str == \"positive\":\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "def measure(actual_task, path, variants, glue_metric):\n",
    "    (field1, field2, field3, field4, system_prompt, matchstr) = task_to_keys[actual_task]\n",
    "    dataset = load_dataset('json', data_files=f\"{path}{variants[0]}.json\")\n",
    "    metric = load_metric('glue', glue_metric)\n",
    "    \n",
    "    prompt = f\"<|system|>{system_prompt}\\n\"\n",
    "    training_data = dataset['train']\n",
    "    \n",
    "    # Here we collect a diverse labled training set of 5 to make a few shot examples\n",
    "    last_label = 0\n",
    "    num_examples = 0\n",
    "    for idx, f1 in enumerate(training_data[field1]):\n",
    "        if training_data['label'][idx] == last_label: continue\n",
    "        prompt += f\"<|{field1 if field1 != \"Sent\" else \"sentence\"}|>{f1}\\n\"\n",
    "        if not field2 is None:\n",
    "            prompt += f\"<|{field2}|>{training_data[field2][idx]}\\n\"\n",
    "        if not field3 is None:\n",
    "            prompt += f\"<|{field3}|>{training_data[field3][idx]}\\n\"\n",
    "        if not field4 is None:\n",
    "            if training_data[field4][idx] == \"cause\":\n",
    "                prompt += f\"<|{field4}|>indok\\n\"\n",
    "            else:\n",
    "                prompt += f\"<|{field4}|>következmény\\n\"\n",
    "        prompt += f\"<|assistant|>{get_label_value(training_data['label'][idx])}\\n\"\n",
    "        \n",
    "        last_label = training_data['label'][idx]\n",
    "        num_examples += 1\n",
    "        if num_examples >= 5: break\n",
    "    \n",
    "    references = []\n",
    "    predictions = []\n",
    "    failed = []\n",
    "    testset = load_dataset('json', data_files=f\"{path}{variants[1]}.json\")\n",
    "    testset = testset['train']\n",
    "    count = 0\n",
    "    for idx, f1 in enumerate(testset[field1]):\n",
    "        query = prompt\n",
    "        query += f\"<|{field1 if field1 != \"Sent\" else \"sentence\"}|>{f1}\\n\"\n",
    "        if not field2 is None:\n",
    "            query += f\"<|{field2}|>{training_data[field2][idx]}\\n\"\n",
    "        if not field3 is None:\n",
    "            query += f\"<|{field3}|>{training_data[field3][idx]}\\n\"\n",
    "        if not field4 is None:\n",
    "            if training_data[field4][idx] == \"cause\":\n",
    "                query += f\"<|{field4}|>indok\\n\"\n",
    "            else:\n",
    "                query += f\"<|{field4}|>következmény\\n\"\n",
    "        query += f\"<|assistant|>\"\n",
    "\n",
    "        result = call_llm(query)\n",
    "        \n",
    "        num_result = find_first_number(result, matchstr)\n",
    "        if num_result is None:\n",
    "            failed.append(idx)\n",
    "        else:\n",
    "            references.append(get_label_value(testset['label'][idx]))\n",
    "            predictions.append(num_result)\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print(f\"Task: {actual_task}, {count/len(testset[field1])*100:5.1f}%\", end=\"\\r\")\n",
    "    \n",
    "    results = metric.compute(predictions=predictions, references=references)\n",
    "    return (len(testset[field1]), len(failed), results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd4271-8728-4f81-bb49-e07f5d2adf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (actual_task, path, variants, glue_metric) in HULU_TASKS:\n",
    "    if actual_task == task:\n",
    "        (number, failed, result) = measure(task, path, variants, glue_metric)\n",
    "        break\n",
    "\n",
    "print(f\"Task: {task} {number}/{failed}               \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35765694-f85f-4a57-a67c-2b9802a9b157",
   "metadata": {},
   "source": [
    "# Putting all together\n",
    "- iterate through all tasks\n",
    "- load related dataset & metric\n",
    "- evaluate all verification elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f26a84fb-03f4-4133-aefa-2bede06d86b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: hucola 910/0               \n",
      "{'matthews_correlation': 0.2671840356811353}\n",
      "Task: hucopa 100/74               \n",
      "{'accuracy': 0.6538461538461539}\n",
      "Task: hucb 103/1               \n",
      "{'accuracy': 0.3431372549019608}\n",
      "Task: hurte 243/0               \n",
      "{'accuracy': 0.4444444444444444}\n",
      "Task: husst 1165/0               \n",
      "{'accuracy': 0.6283261802575107}\n",
      "Task: huwnli 60/0               \n",
      "{'accuracy': 0.5333333333333333}\n"
     ]
    }
   ],
   "source": [
    "for (task, path, variants, glue_metric) in HULU_TASKS:\n",
    "    (number, failed, result) = measure(task, path, variants, glue_metric)\n",
    "    print(f\"Task: {task} {number}/{failed}               \")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf759954-8158-4d50-abf9-d86fb561f4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
