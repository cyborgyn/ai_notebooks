{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f721eb44-fc10-429b-8983-281543317af8",
   "metadata": {},
   "source": [
    "# Generating an English - Hungarian translation test set\n",
    "First of all **WHY**? Why do I want to have a dataset of translation pairs?\n",
    "\n",
    "Since I intend to use it to evaluate LLM translation capabilities, it does make sence to use a few (tousend) translation examples to instruct the LLM to complete, and evaluate against. Decideing if a sentence pair is each other's translation, is easy even for models not \"speaking\" Hungarian well. However, producing those translations are not so.\n",
    "\n",
    "**HOW MUCH** of it do we need?\n",
    "\n",
    "We need a quickly evaluatable set (on the order of few 1000s), from the broadest possible sources. Diversity here really helps.\n",
    "For first tests I decided to go with 2000 sentence pairs, represented the same way as it is in the original source:\n",
    "- 182 from classic literature\n",
    "- 1508 from modern literature\n",
    "- 310 from subtitles\n",
    "\n",
    "For this, we will use some publicly available bilingual corpuses, the Hunglish corpus collection by BME. All the texts are already broke down to English <-> Hungarian sentence pairs.\n",
    "\n",
    "It consists of several sources:\n",
    "- Classical literature book pairs (202744) 9.1%\n",
    "- Modern literature book pairs (scrambled text lines) (1670129) 75.4%\n",
    "- Movie subtitles paired up (343331) 15.5%\n",
    "\n",
    "## Observations on the source data\n",
    "Just peeking into the data randomly here and there, I realized that:\n",
    "- **There are misspellings** all around the data. We can mittigate this by using a spell checking on the data before importing it into our set. \n",
    "- **There are numerous misspairing** / mistranslations of sentences all around the source dataset. To solve this: we will use the best HuLU scored open source LLM, Gemma2 to just decide if 2 sentences are translations of each other. This will most probably filter out a lot of good translations as well, which Gemma2 wouldn't understand correctly, but it's still better to have a simpler but mostly correct dataset to work from.\n",
    "- **Meaningless pairs** here and there. Like only numbers translated to numbers. There is no language specific translation involved in them. We will put a minimum word count on sentences on both sides to mittigate this.\n",
    "- **Incorrect characters** are used for őŐ and űŰ. They will need to be replaced (õ->ő, û->Ű).\n",
    "- **Duplicates** are there, especially for the short sentences, and in the subtitles. We will need to do a deduplication on the ready dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b3c014-a01f-49ad-8abb-0de956fd3579",
   "metadata": {},
   "source": [
    "## Program settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3b77757-4953-4824-b206-fde3dd515f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "target_count = 2000\n",
    "sources = [\n",
    "    (\"classic_utf8.bi\", int(target_count * 0.091)),\n",
    "    (\"modern_utf8.bi\", int(target_count * 0.754)),\n",
    "    (\"subtitles_utf8.bi\", int(target_count * 0.155))\n",
    "]\n",
    "\n",
    "class BiLingualData:\n",
    "    def __init__(self, id = \"\", english = \"\", hungarian = \"\"):\n",
    "        self.id = id\n",
    "        self.english = english\n",
    "        self.hungarian = hungarian\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self.__dict__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bdf780-790a-4118-83c0-877364d67d12",
   "metadata": {},
   "source": [
    "## Helper method definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ecd8ff40-d62e-4eed-892d-3e7f7088d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes excess whitespace chars, replaces incorrect chars, and trims\n",
    "def TrimSentence(sentence):\n",
    "    while \"  \" in sentence:\n",
    "        sentence = sentence.replace(\"  \", \" \")\n",
    "    sentence = sentence.replace(\"õ\", \"ő\").replace(\"û\", \"ű\")\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "87c7ffc1-77fe-4b85-845c-614fbf4b36d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "# Checks for spelling errors in the sentence. Returns true if no errors were found\n",
    "def SpellCheckHU(sentence):\n",
    "    # Run the bash command with the provided input string\n",
    "    result = subprocess.run(\"hunspell -d hu-HU -l\", input=sentence, capture_output=True, text=True, shell=True)\n",
    "    # Capture the output\n",
    "    outputlines = result.stdout.splitlines()\n",
    "    return len(outputlines) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a04672d8-02e6-4160-a366-6eb97bec6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def SpellCheckEN(sentence):\n",
    "    # Run the bash command with the provided input string\n",
    "    result = subprocess.run(\"hunspell -d en-US -l\", input=sentence, capture_output=True, text=True, shell=True)\n",
    "    # Capture the output\n",
    "    outputlines = result.stdout.splitlines()\n",
    "    return len(outputlines) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "338e9572-c3a4-4465-a2f5-769bfa63d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordCount(sentence):\n",
    "    return len(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "921553ee-a83a-488b-84ee-b11189beae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "def are_pairs_correct(english, hungarian):\n",
    "    api_url = \"http://localhost:5001/api/v1\"\n",
    "    stop_words = [\"###\",\"</s>\",\"<|\"]\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"prompt\": f\"\"\"<|system|>Determine if the English and Hungarian sentence pair is translations of each other (1), or not (0).<|end|>\n",
    "<|english|>What time is it?<|end|>\n",
    "<|hungarian|>Mennyi az idő?<|end|>\n",
    "<|assistant|>1<|end|>\n",
    "<|english|>The surface is the darkest among Uranian moons, and appears to have been shaped primarily by impacts.<|end|>\n",
    "<|hungarian|>A felszíne a legsötétebb az uránuszi holdak közül, és úgy tűnik, leginkább becsapódások alakították.<|end|>\n",
    "<|assistant|>1<|end|>\n",
    "<|english|>Umbriel, along with another Uranian satellite, Ariel, was discovered by William Lassell on October 24, 1851.<|end|>\n",
    "<|hungarian|>Umbrielt a többi uránuszi holddal együtt a Voyager 2 űrszonda vizsgálta, 1986 januárjában.<|end|>\n",
    "<|assistant|>0<|end|>\n",
    "<|english|>He created numerous programs to provide relief to the unemployed and farmers while seeking economic recovery with the National Recovery Administration and other programs.<|end|>\n",
    "<|hungarian|>Számos programot hozott létre a munkanélküliek és gazdálkodók megsegítésére, miközben az Országos Helyreállítási Igazgatósággal és más programokkal kereste a gazdasági fellendülést.<|end|>\n",
    "<|assistant|>1<|end|>\n",
    "<|english|>{english}<|end|>\n",
    "<|hungarian|>{hungarian}<|end|>\n",
    "<|assistant|>\n",
    "\"\"\",\n",
    "        \"max_tokens\": 10,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"n\": 20,\n",
    "        \"stop\": stop_words\n",
    "    }\n",
    "    \n",
    "    response = requests.post(f\"{api_url}/completion\", headers=headers, json=data)\n",
    "    result = response.json()[\"choices\"][0][\"text\"]\n",
    "    for sw in stop_words:\n",
    "        result = result.replace(sw, \"\")\n",
    "    match = re.search(r'[01]', result)\n",
    "    return int(match.group()) == 1 if match else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa9a872f-eb6e-4552-868d-06c5ccb64665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:03:25.80,00:03:28.48 Ezt a halála napján írta be.\n",
      "spell hu: True\n",
      "word cnt: 7\n",
      "are translations: True\n"
     ]
    }
   ],
   "source": [
    "# Small simple test harness\n",
    "\n",
    "line = \"00:03:25.80,00:03:28.48 Ezt a halála napján írta be.\tAnd that was dated the day he died.\"\n",
    "(hun, eng) = line.split(\"\\t\")\n",
    "hun = TrimSentence(hun)\n",
    "print(hun)\n",
    "eng = TrimSentence(eng)\n",
    "print(f\"spell hu: {SpellCheckHU(hun)}\")\n",
    "print(f\"word cnt: {WordCount(hun)}\")\n",
    "print(f\"are translations: {are_pairs_correct(eng, hun)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4392ad9-54c6-459a-bfe8-ff4a7b862ea6",
   "metadata": {},
   "source": [
    "## Process input data\n",
    "What is defined in the ```sources``` variable, we iterate through, and get random lines until we meet the required target number of lines to import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4ca16b81-f15a-4094-8a5c-c4eb52e76354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.                    \n",
      "Skipped lines: 4597\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "all_data = []\n",
    "master_count = 0\n",
    "skipped = 0\n",
    "for (file, target_count_per_file) in sources:\n",
    "    with open(file, 'r', encoding='utf-8') as content:\n",
    "        all_lines = content.read()\n",
    "    lines = all_lines.splitlines()\n",
    "\n",
    "    picks = []\n",
    "    filedata = []\n",
    "    while len(filedata) < target_count_per_file:\n",
    "        # Pick a line randomly (which wasn't picked before)\n",
    "        pick = random.randint(0, len(lines)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(lines)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "        # Check if line is OK for us\n",
    "        line = lines[pick]\n",
    "        (hun, eng) = line.split(\"\\t\")\n",
    "        hun = TrimSentence(hun)\n",
    "        eng = TrimSentence(eng)\n",
    "        if WordCount(hun) > 4 and WordCount(eng) > 4 and SpellCheckHU(hun) and are_pairs_correct(eng, hun):\n",
    "            master_count += 1\n",
    "            data = BiLingualData(id=master_count, english=eng, hungarian=hun)\n",
    "            filedata.append(data)\n",
    "            all_data.append(data)\n",
    "\n",
    "            if master_count % 10 == 0:\n",
    "                print(f\"Processing {master_count/target_count*100:5.1f}%\", end=\"\\r\")\n",
    "        else:\n",
    "            skipped += 1\n",
    "\n",
    "first = True\n",
    "with open(\"hunglish-BLEU.json\", 'w', encoding='utf-8') as writer:\n",
    "    writer.write(\"[\")\n",
    "    for data in all_data:\n",
    "        if not first:\n",
    "            writer.write(\",\")\n",
    "        first = False\n",
    "        writer.write(data.to_json())\n",
    "    writer.write(\"]\")\n",
    "\n",
    "print(\"Ready.                    \")\n",
    "print(f\"Skipped lines: {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd078f5-c7bf-4e17-985c-6d0acd6894bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
