{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b605c15-fe38-4905-96d7-6fc9301966ba",
   "metadata": {},
   "source": [
    "# GLUE eval of Koboldcpp hosted LLM\n",
    "Install required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192ee022-1506-4633-8d58-e5ae2a306d39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f28fe92-34a5-4ccd-83d9-bbe414917fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install datasets evaluate requests scipy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f329a8a-934e-4916-9b7f-d7231f85e5e0",
   "metadata": {},
   "source": [
    "Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f66cb71-794e-490a-b64a-9adb67ea03ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40349319-3430-4f9c-93f8-d2b6c3968c6c",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to fine-tune one of the [Transformers](https://github.com/huggingface/transformers) model to a text classification task of the [GLUE Benchmark](https://gluebenchmark.com/).\n",
    "\n",
    "The GLUE Benchmark is a group of nine classification tasks on sentences or pairs of sentences which are:\n",
    "\n",
    "- [CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability) Determine if a sentence is grammatically correct or not.is a  dataset containing sentences labeled grammatically correct or not.\n",
    "- [MNLI](https://arxiv.org/abs/1704.05426) (Multi-Genre Natural Language Inference) Determine if a sentence entails, contradicts or is unrelated to a given hypothesis. (This dataset has two versions, one with the validation and test set coming from the same distribution, another called mismatched where the validation and test use out-of-domain data.)\n",
    "- [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (Microsoft Research Paraphrase Corpus) Determine if two sentences are paraphrases from one another or not.\n",
    "- [QNLI](https://rajpurkar.github.io/SQuAD-explorer/) (Question-answering Natural Language Inference) Determine if the answer to a question is in the second sentence or not. (This dataset is built from the SQuAD dataset.)\n",
    "- [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora Question Pairs2) Determine if two questions are semantically equivalent or not.\n",
    "- [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) (Recognizing Textual Entailment) Determine if a sentence entails a given hypothesis or not.\n",
    "- [SST-2](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank) Determine if the sentence has a positive or negative sentiment.\n",
    "- [STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) (Semantic Textual Similarity Benchmark) Determine the similarity of two sentences with a score from 1 to 5.\n",
    "- [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html) (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. (This dataset is built from the Winograd Schema Challenge dataset.)\n",
    "\n",
    "We will see how to easily load the dataset for each one of those tasks and use the `Trainer` API to fine-tune a model on it. Each task is named by its acronym, with `mnli-mm` standing for the mismatched version of MNLI (so same training set as `mnli` but different validation and test sets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b45377c7-7f69-4c11-90ff-b99a3f9e907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]\n",
    "#GLUE_TASKS = [\"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]\n",
    "task = \"qqp\"\n",
    "\n",
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeeb854-c914-443e-8d0d-becdd3d6079c",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ec648-694f-4662-820e-43fb6f272f1d",
   "metadata": {},
   "source": [
    "We will use the [Datasets](https://github.com/huggingface/datasets) library to download the data and [Evaluate](https://github.com/huggingface/evaluate) library to get the metric we need to use for evaluation (to compare our model to the benchmark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c317c8df-c8f6-4bba-95aa-3eb017f02517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 363846\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 40430\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 390965\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecea3095-0c3d-4bb3-b65e-a34a41de204e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.75, 'f1': 0.8}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load as load_metric\n",
    "metric = load_metric('glue', actual_task)\n",
    "\n",
    "print(\"example output\")\n",
    "references = [0, 1, 0, 1]\n",
    "predictions = [0, 1, 1, 1]\n",
    "\n",
    "\n",
    "results = metric.compute(predictions=predictions, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ac3b9-033a-4929-bdb6-be28ca70dfdb",
   "metadata": {},
   "source": [
    "## Output values\n",
    "\n",
    "The output of the metric depends on the GLUE subset chosen, consisting of a dictionary that contains one or several of the following metrics:\n",
    "\n",
    "`accuracy`: the proportion of correct predictions among the total number of cases processed, with a range between 0 and 1 (see [accuracy](https://huggingface.co/metrics/accuracy) for more information).\n",
    "\n",
    "`f1`: the harmonic mean of the precision and recall (see [F1 score](https://huggingface.co/metrics/f1) for more information). Its range is 0-1 â€“ its lowest possible value is 0, if either the precision or the recall is 0, and its highest possible value is 1.0, which means perfect precision and recall.\n",
    "\n",
    "`pearson`: a measure of the linear relationship between two datasets (see [Pearson correlation](https://huggingface.co/metrics/pearsonr) for more information). Its range is between -1 and +1, with 0 implying no correlation, and -1/+1 implying an exact linear relationship. Positive correlations imply that as x increases, so does y, whereas negative correlations imply that as x increases, y decreases.\n",
    "\n",
    "`spearmanr`: a nonparametric measure of the monotonicity of the relationship between two datasets(see [Spearman Correlation](https://huggingface.co/metrics/spearmanr) for more information). spearmanr has the same range as pearson.\n",
    "\n",
    "`matthews_correlation`: a measure of the quality of binary and multiclass classifications (see [Matthews Correlation](https://huggingface.co/metrics/matthews_correlation) for more information). Its range of values is between -1 and +1, where a coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction.\n",
    "\n",
    "The cola subset returns matthews_correlation, the stsb subset returns pearson and spearmanr, the mrpc and qqp subsets return both accuracy and f1, and all other subsets of GLUE return only accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d51fa5-850d-4e5a-9867-d03654a09a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: f\"{typ.names[i]} ({i})\")\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7aca74-3887-4565-9d3b-a5081df6897f",
   "metadata": {},
   "source": [
    "We can visualize a random portion of the loaded dataset by calling the above defined method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f5ebcc-d93a-49d9-9bdf-95389cbe35c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I improve my data analysis skills?</td>\n",
       "      <td>How do I improve my research analysis and data analysis skills?</td>\n",
       "      <td>duplicate (1)</td>\n",
       "      <td>170446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is SAP?</td>\n",
       "      <td>What is sap bi?</td>\n",
       "      <td>not_duplicate (0)</td>\n",
       "      <td>140876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Will the electoral college change their vote if enough people protest against Donald Trump?</td>\n",
       "      <td>Do you think republican members of the electoral college will go against tradition and refuse to vote for Trump in December 2016?</td>\n",
       "      <td>duplicate (1)</td>\n",
       "      <td>329463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What will be a better choice between XLRI and IIFT?</td>\n",
       "      <td>Which is better XLRI or IIFT?</td>\n",
       "      <td>duplicate (1)</td>\n",
       "      <td>47384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do cameras work? How have they improved since their creation?</td>\n",
       "      <td>How do cameras work?</td>\n",
       "      <td>duplicate (1)</td>\n",
       "      <td>185125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What happens if your car runs out of engine coolant?</td>\n",
       "      <td>How do I tell if my car is leaking coolant?</td>\n",
       "      <td>not_duplicate (0)</td>\n",
       "      <td>162766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Which topics I should cover in air wave propagation?</td>\n",
       "      <td>Why does sound need a medium like air or water in order to travel, but radio waves do not?</td>\n",
       "      <td>not_duplicate (0)</td>\n",
       "      <td>186856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What are some yoga poses to help me lose weight?</td>\n",
       "      <td>What are the best yoga poses for weight loss?</td>\n",
       "      <td>duplicate (1)</td>\n",
       "      <td>120778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How do block pornsites and pornwords on Chrome is?</td>\n",
       "      <td>If the United States goes to war with another country, what happens to its own citizens residing in that country or in another country nearby?</td>\n",
       "      <td>not_duplicate (0)</td>\n",
       "      <td>281613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How can I earn money from Facebook page?</td>\n",
       "      <td>How do I earn money with my Facebook page?</td>\n",
       "      <td>duplicate (1)</td>\n",
       "      <td>60637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3197b46-9236-4d21-b574-57c3304b04db",
   "metadata": {},
   "source": [
    "## Task description\n",
    "Here we define a task description for each of the tasks:\n",
    "- Where the data can be found in the dataset\n",
    "- What needs the LLM do with those\n",
    "\n",
    "Out of these a prompt will be constructed for each of the tasks, automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aff3bc6-ffc2-4caf-ab90-c966e89eb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None, \"Determine if a sentence is grammatically correct (1) or not (0).\", \"validation\", r'[01]'),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\", \"Determine if a sentence entails (0), contradicts (1) or is unrelated (2) to a given hypothesis.\", \"validation_matched\", r'[012]'),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\", \"Determine if two sentences are paraphrases from one another (1) or not (0).\", \"validation\", r'[01]'),\n",
    "    \"qnli\": (\"question\", \"sentence\", \"Determine if the answer to a question is in the second sentence (0) or not (1).\", \"validation\", r'[01]'),\n",
    "    \"qqp\": (\"question1\", \"question2\", \"Determine if two questions are semantically equivalent (1) or not (0).\", \"validation\", r'[01]'),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\", \"Determine if a sentence entails a given hypothesis (0) or not (1).\", \"validation\", r'[01]'),\n",
    "    \"sst2\": (\"sentence\", None, \"Determine if the sentence has a positive (1) or negative (0) sentiment.\", \"validation\", r'[01]'),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\", \"Rate the semantic similarity between two sentences with a numeric score between 0.0 to 5.0.\", \"validation\", r'\\d+(\\.\\d+)?'),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\", \"Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed (1) or not (0).\", \"validation\", r'[01]'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a981d21-ab3c-4c38-a547-6ce5c24ffb4a",
   "metadata": {},
   "source": [
    "## The prompt\n",
    "To prepare the prompt, we provide a system description, comming from `task_to_keys`, and additional 5 examples from the training dataset, with scores included.\n",
    "\n",
    "The actual test is comming from the \"validation\" dataset part, one by one, only providing the sentences to operate on, and simply prompting for a numerical answer by placing <|assistant|> at the line start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e4b095-ba36-4fc5-b5e9-99d67f55fbec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def find_first_number(input_str, matchstr):\n",
    "    match = re.search(matchstr, input_str)\n",
    "    return float(match.group()) if match else None\n",
    "\n",
    "def measure(actual_task, dataset, metric):\n",
    "    (field1, field2, system_prompt, dsetname, matchstr) = task_to_keys[actual_task]\n",
    "    prompt = f\"<|system|>{system_prompt}\\n\"\n",
    "    training_data = dataset['train']\n",
    "    \n",
    "    # Here we collect a diverse labled training set of 5 to make a few shot examples\n",
    "    last_label = 0\n",
    "    num_examples = 0\n",
    "    for idx, f1 in enumerate(training_data[field1]):\n",
    "        if training_data['label'][idx] == last_label: continue\n",
    "        prompt += f\"<|{field1}|>{f1}\\n\"\n",
    "        if not field2 is None:\n",
    "            prompt += f\"<|{field2}|>{training_data[field2][idx]}\\n\"\n",
    "        prompt += f\"<|assistant|>{training_data['label'][idx]}\\n\"\n",
    "        \n",
    "        last_label = training_data['label'][idx]\n",
    "        num_examples += 1\n",
    "        if num_examples >= 5: break\n",
    "    \n",
    "    api_url = \"http://localhost:5001/api/v1\"\n",
    "    stop_words = [\"###\",\"**Observation**\",\"</s>\",\"<|\"]\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    references = []\n",
    "    predictions = []\n",
    "    failed = []\n",
    "    testset = dataset[dsetname]\n",
    "    count = 0\n",
    "    for idx, f1 in enumerate(testset[field1]):\n",
    "        query = prompt\n",
    "        query += f\"<|{field1}|>{f1}\\n\"\n",
    "        if not field2 is None:\n",
    "            query += f\"<|{field2}|>{testset[field2][idx]}\\n\"\n",
    "        query += f\"<|assistant|>\"\n",
    "    \n",
    "        data = {\n",
    "            \"prompt\": query,\n",
    "            \"max_tokens\": 10,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"n\": 20,\n",
    "            \"stop\": stop_words\n",
    "        }\n",
    "        \n",
    "        response = requests.post(f\"{api_url}/completion\", headers=headers, json=data)\n",
    "        result = response.json()[\"choices\"][0][\"text\"]\n",
    "        for sw in stop_words:\n",
    "            result = result.replace(sw, \"\")\n",
    "        result = result.strip()\n",
    "        num_result = find_first_number(result, matchstr)\n",
    "        if num_result is None:\n",
    "            failed.append(idx)\n",
    "        else:\n",
    "            references.append(testset['label'][idx])\n",
    "            predictions.append(num_result)\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print(f\"Task: {actual_task}, {count/len(testset[field1])*100:5.1f}%\", end=\"\\r\")\n",
    "    \n",
    "    results = metric.compute(predictions=predictions, references=references)\n",
    "    return (len(testset[field1]), len(failed), results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbbd4271-8728-4f81-bb49-e07f5d2adf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: mrpc 100/0               \n",
      "{'accuracy': 0.74, 'f1': 0.8333333333333334}\n"
     ]
    }
   ],
   "source": [
    "(number, failed, result) = measure(task, dataset, metric)\n",
    "print(f\"Task: {task} {number}/{failed}               \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35765694-f85f-4a57-a67c-2b9802a9b157",
   "metadata": {},
   "source": [
    "# Putting all together\n",
    "- iterate through all tasks\n",
    "- load related dataset & metric\n",
    "- evaluate all verification elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f26a84fb-03f4-4133-aefa-2bede06d86b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: cola 1043/1               \n",
      "{'matthews_correlation': 0.45562723707910674}\n",
      "Task: mnli 9815/0               \n",
      "{'accuracy': 0.3869587366276108}\n",
      "Task: mrpc 408/0               \n",
      "{'accuracy': 0.7549019607843137, 'f1': 0.8355263157894737}\n",
      "Task: qnli 5463/0               \n",
      "{'accuracy': 0.13637195680029288}\n",
      "Task: qqp 40430/0               \n",
      "{'accuracy': 0.8235221370269602, 'f1': 0.7773304621914303}\n",
      "Task: rte 277/0               \n",
      "{'accuracy': 0.35379061371841153}\n",
      "Task: sst2 872/0               \n",
      "{'accuracy': 0.9357798165137615}\n",
      "Task: stsb 1500/0               \n",
      "{'pearson': 0.826388381403035, 'spearmanr': 0.8408428920714652}\n",
      "Task: wnli 71/0               \n",
      "{'accuracy': 0.7183098591549296}\n"
     ]
    }
   ],
   "source": [
    "for task in GLUE_TASKS:\n",
    "    dataset = load_dataset(\"glue\", task)\n",
    "    metric = load_metric('glue', task)\n",
    "    (number, failed, result) = measure(task, dataset, metric)\n",
    "    print(f\"Task: {task} {number}/{failed}               \")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf759954-8158-4d50-abf9-d86fb561f4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
